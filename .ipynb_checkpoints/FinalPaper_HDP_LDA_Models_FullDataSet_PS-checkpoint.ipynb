{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fadcc6c-c439-42ed-9780-bf1aead06cb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T20:20:49.734686Z",
     "iopub.status.busy": "2024-03-29T20:20:49.734398Z",
     "iopub.status.idle": "2024-03-29T20:21:16.450943Z",
     "shell.execute_reply": "2024-03-29T20:21:16.449912Z",
     "shell.execute_reply.started": "2024-03-29T20:20:49.734665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfminer\n",
      "  Downloading pdfminer-20191125.tar.gz (4.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pdfminer.six\n",
      "  Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gensim\n",
      "  Downloading gensim-4.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting wordcloud\n",
      "  Downloading wordcloud-1.9.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (513 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m513.6/513.6 kB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.9.2)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.12.1+cu116)\n",
      "Collecting pycryptodome\n",
      "  Downloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cryptography>=36.0.0\n",
      "  Downloading cryptography-42.0.5-cp39-abi3-manylinux_2_28_x86_64.whl (4.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from pdfminer.six) (2.1.1)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in /usr/local/lib/python3.9/dist-packages (from PyPDF2) (4.4.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.9.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.23.4)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from wordcloud) (9.2.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from wordcloud) (3.6.1)\n",
      "Collecting pypdfium2>=4.18.0\n",
      "  Downloading pypdfium2-4.28.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.9.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.30.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.51.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (66.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (15.0.6.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.35.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.9/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.15.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.28.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.16.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (4.38.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (1.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (6.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2019.11.28)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.2)\n",
      "Building wheels for collected packages: pdfminer\n",
      "  Building wheel for pdfminer (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pdfminer: filename=pdfminer-20191125-py3-none-any.whl size=6140081 sha256=92d2e0a379125036ae5826417cefe3bdc1a7696a58207ab65fa823aabd465a08\n",
      "  Stored in directory: /root/.cache/pip/wheels/41/44/38/6f82831ef593d74608e2f3cca841d5b43b8eb31b6cd60560a9\n",
      "Successfully built pdfminer\n",
      "Installing collected packages: pypdfium2, PyPDF2, pycryptodome, pdfminer, gensim, cryptography, wordcloud, pdfminer.six, pdfplumber\n",
      "Successfully installed PyPDF2-3.0.1 cryptography-42.0.5 gensim-4.3.2 pdfminer-20191125 pdfminer.six-20231228 pdfplumber-0.11.0 pycryptodome-3.20.0 pypdfium2-4.28.0 wordcloud-1.9.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfminer pdfminer.six PyPDF2 gensim wordcloud pdfplumber tensorflow torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0d4fbf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T20:21:16.452889Z",
     "iopub.status.busy": "2024-03-29T20:21:16.452572Z",
     "iopub.status.idle": "2024-03-29T20:21:21.497868Z",
     "shell.execute_reply": "2024-03-29T20:21:21.496995Z",
     "shell.execute_reply.started": "2024-03-29T20:21:16.452870Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing relevant libraries\n",
    "from pdfminer.high_level import extract_text\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import download\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel\n",
    "import os\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pdfplumber\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import HdpModel\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "# Download other resources\n",
    "download('stopwords')\n",
    "download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc21eff2-bf0e-42bc-b97a-78c0aa7de8d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T20:21:21.499849Z",
     "iopub.status.busy": "2024-03-29T20:21:21.498923Z",
     "iopub.status.idle": "2024-03-29T20:21:22.879123Z",
     "shell.execute_reply": "2024-03-29T20:21:22.878257Z",
     "shell.execute_reply.started": "2024-03-29T20:21:21.499823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define a tensor\n",
    "tensor = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Using GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using CPU')\n",
    "\n",
    "# Move your data to the GPU if available\n",
    "tensor_on_gpu = tensor.to(device)\n",
    "\n",
    "# Perform computations using GPU-accelerated operations\n",
    "result = tensor_on_gpu + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a2d4c38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T20:21:22.880166Z",
     "iopub.status.busy": "2024-03-29T20:21:22.879966Z",
     "iopub.status.idle": "2024-03-29T20:21:22.914378Z",
     "shell.execute_reply": "2024-03-29T20:21:22.913589Z",
     "shell.execute_reply.started": "2024-03-29T20:21:22.880148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files in datasource:  1971\n"
     ]
    }
   ],
   "source": [
    "#Initial stats - file count\n",
    "\n",
    "def count_files_in_folder(folder_path):\n",
    "    # Initialize a counter for files\n",
    "    file_count = 0\n",
    "\n",
    "    # Walk through the directory and count files\n",
    "    for _, _, files in os.walk(folder_path):\n",
    "        file_count += len(files)\n",
    "\n",
    "    return file_count\n",
    "\n",
    "folder_path = '/notebooks/CIND820/Datasource'\n",
    "total_files = count_files_in_folder(folder_path)\n",
    "print(\"Total files in datasource: \", total_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94d251db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T20:21:22.916293Z",
     "iopub.status.busy": "2024-03-29T20:21:22.916077Z",
     "iopub.status.idle": "2024-03-29T20:22:13.411231Z",
     "shell.execute_reply": "2024-03-29T20:22:13.410259Z",
     "shell.execute_reply.started": "2024-03-29T20:21:22.916275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages in all PDF files: 155385\n",
      "Mean page count per file: 78.8756345177665\n",
      "Median page count per file: 80.0\n"
     ]
    }
   ],
   "source": [
    "#Initial stats - page count\n",
    "\n",
    "def count_pages_and_stats(folder_path):\n",
    "    total_pages = 0\n",
    "    page_counts = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdf'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'rb') as file:\n",
    "                pdf_reader = PdfReader(file)\n",
    "                num_pages = len(pdf_reader.pages)\n",
    "                total_pages += num_pages\n",
    "                page_counts.append(num_pages)\n",
    "\n",
    "    mean_page_count = statistics.mean(page_counts)\n",
    "    median_page_count = statistics.median(page_counts)\n",
    "\n",
    "    return total_pages, mean_page_count, median_page_count\n",
    "\n",
    "folder_path = '/notebooks/CIND820/Datasource'\n",
    "total_pages, mean_page_count, median_page_count = count_pages_and_stats(folder_path)\n",
    "\n",
    "print(\"Total pages in all PDF files:\", total_pages)\n",
    "print(\"Mean page count per file:\", mean_page_count)\n",
    "print(\"Median page count per file:\", median_page_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "234aa441",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T20:22:13.412655Z",
     "iopub.status.busy": "2024-03-29T20:22:13.412426Z",
     "iopub.status.idle": "2024-03-29T20:22:16.416543Z",
     "shell.execute_reply": "2024-03-29T20:22:16.415860Z",
     "shell.execute_reply.started": "2024-03-29T20:22:13.412637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_54/2350619243.py:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "Using device: /GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Set device to CUDA if available\n",
    "device = '/GPU:0' if tf.test.is_gpu_available() else '/CPU:0'\n",
    "\n",
    "# Function for preprocessing text\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    french_stopwords = set(stopwords.words('french'))\n",
    "    stop_words.update(french_stopwords)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Remove specific words or letters which are not useful\n",
    "    additional_stopwords = [\n",
    "        'mr.', 'mr', 'mrs.', 'ms.', 'speaker', 'bill', 'debate', 'hon', 'cpc', 'lib', 'bq', 'canadian', \n",
    "        'act', 'amend', 'amendment', 'canada', 'house', 'public', 'honour', 'minister', 'ministry', 'govern', \n",
    "        'member', 'program', 'primeminister', 'would', 'people', 'chair', 'committe', 'liber', 'polici', 'parliamentari', \n",
    "        'ndp', 'government', 'conserv', 'parties', 'partisan', 's', 'b', 'c', 'e', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'o', 'p',\n",
    "        'q', 'r', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'am', 'pm','year','time','motion','go', 'canadians', \n",
    "        'also', 'members', 'madam', 'committee', 'prime', 'senate', 'senator', 'hous',\n",
    "        'one', 'govern', 'liberal', 'conservative', 'liberals', 'conservatives', 'speech', 'parliamentarian',\n",
    "        'secretariat', 'ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'x', 'xi', '000', '1', '3', '5', '11', \n",
    "        '15', '22', '25', '2007', '2008', '2009', '2010', '2011',\n",
    "        '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023', '‚Äô',\n",
    "        '‚Äú', '‚Äù', \"’\",'...................',' ................................................',\n",
    "        '........',\"'s\" ]\n",
    "    stop_words.update(additional_stopwords)\n",
    "    tokens = [token for token in tokens if token not in additional_stopwords]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Directory path containing PDF files\n",
    "pdf_directory = '/notebooks/CIND820/Datasource'\n",
    "\n",
    "# List all PDF files in the directory\n",
    "pdf_files = [os.path.join(pdf_directory, file) for file in os.listdir(pdf_directory) if file.endswith('.pdf')]\n",
    "\n",
    "texts = []\n",
    "\n",
    "# Function to process a single PDF file\n",
    "def process_pdf(pdf_file):\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Preprocess text\n",
    "preprocessed_texts = [preprocess_text(text) for text in texts]\n",
    "\n",
    "# Create a dictionary from the preprocessed text\n",
    "dictionary = Dictionary(preprocessed_texts)\n",
    "\n",
    "# Create a corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in preprocessed_texts]\n",
    "\n",
    "# Convert corpus and dictionary to TensorFlow tensors\n",
    "with tf.device(device):\n",
    "    corpus_tensor = tf.convert_to_tensor(corpus)\n",
    "    dictionary_tensor = tf.convert_to_tensor(dictionary)\n",
    "print('Using device:', device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd108981",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T20:22:16.417966Z",
     "iopub.status.busy": "2024-03-29T20:22:16.417674Z",
     "iopub.status.idle": "2024-03-29T20:22:16.424018Z",
     "shell.execute_reply": "2024-03-29T20:22:16.423321Z",
     "shell.execute_reply.started": "2024-03-29T20:22:16.417936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The corpus is empty. Please provide a non-empty corpus.\n"
     ]
    }
   ],
   "source": [
    "# Check if the corpus has enough samples for splitting\n",
    "if len(corpus_tensor) == 0:\n",
    "    print(\"Error: The corpus is empty. Please provide a non-empty corpus.\")\n",
    "else:\n",
    "    # Split the corpus_tensor into train and test sets\n",
    "    train_corpus, test_corpus = train_test_split(corpus_tensor.numpy(), test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create datasets from train and test sets\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_corpus)\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices(test_corpus)\n",
    "\n",
    "    # Shuffle the train dataset\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=len(train_corpus), seed=42)\n",
    "\n",
    "    # Print the number of samples in each set\n",
    "    print(\"Training Corpus Size:\", len(list(train_dataset)))\n",
    "    print(\"Testing Corpus Size:\", len(list(test_dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df7c7aa2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T20:22:16.425707Z",
     "iopub.status.busy": "2024-03-29T20:22:16.425435Z",
     "iopub.status.idle": "2024-03-29T20:22:16.597923Z",
     "shell.execute_reply": "2024-03-29T20:22:16.596597Z",
     "shell.execute_reply.started": "2024-03-29T20:22:16.425684Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the HDP model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m hdp_model \u001b[38;5;241m=\u001b[39m HdpModel(\u001b[43mtrain_corpus\u001b[49m, id2word\u001b[38;5;241m=\u001b[39mdictionary)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "# Train the HDP model\n",
    "hdp_model = HdpModel(train_corpus, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589839bf",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T20:22:16.598602Z",
     "iopub.status.idle": "2024-03-29T20:22:16.598827Z",
     "shell.execute_reply": "2024-03-29T20:22:16.598734Z",
     "shell.execute_reply.started": "2024-03-29T20:22:16.598722Z"
    }
   },
   "outputs": [],
   "source": [
    "#First evaluation of HDP model and number of topics identified per document)\n",
    "rows = []\n",
    "\n",
    "# Iterate through each document in the corpus\n",
    "for i, doc in enumerate(corpus_tensor):\n",
    "    doc_topics = hdp_model(doc)\n",
    "    # Extract topic numbers and their probabilities\n",
    "    topic_numbers = [topic[0] for topic in doc_topics]\n",
    "    topic_probs = [topic[1] for topic in doc_topics]\n",
    "    # Append the document's topics to the rows list\n",
    "    rows.append([i, topic_numbers, topic_probs])\n",
    "\n",
    "# Create a DataFrame from the list of rows\n",
    "doc_topics_df = pd.DataFrame(rows, columns=['Document_Index', 'Topic_Numbers', 'Topic_Probabilities'])\n",
    "\n",
    "# Display the DataFrame\n",
    "doc_topics_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8c96c0",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T20:22:16.599635Z",
     "iopub.status.idle": "2024-03-29T20:22:16.599836Z",
     "shell.execute_reply": "2024-03-29T20:22:16.599749Z",
     "shell.execute_reply.started": "2024-03-29T20:22:16.599739Z"
    }
   },
   "outputs": [],
   "source": [
    "# To find an approximate number of total topics identified within the HDP model,\n",
    "# train an LDA model using the HDP model as a training mechanism\n",
    "\n",
    "# Move the HDP model to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    hdp_model.to('cuda')\n",
    "\n",
    "# Train an LDA model using the HDP model\n",
    "lda_model_t = hdp_model.suggested_lda_model()\n",
    "\n",
    "# Convert the corpus to PyTorch tensors and move to GPU if available\n",
    "corpus_tensor = torch.tensor(corpus)\n",
    "if torch.cuda.is_available():\n",
    "    corpus_tensor = corpus_tensor.to('cuda')\n",
    "\n",
    "# Get the topic distributions for each document\n",
    "doc_topics = [lda_model_t.get_document_topics(doc) for doc in corpus_tensor]\n",
    "\n",
    "# Count the number of unique topics\n",
    "unique_topics = set()\n",
    "for doc_topics in doc_topics:\n",
    "    unique_topics.update([topic[0] for topic in doc_topics])\n",
    "\n",
    "num_topics_identified = len(unique_topics)\n",
    "print(f\"Number of topics identified by HDP model: {num_topics_identified}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2f2209",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T20:22:16.600706Z",
     "iopub.status.idle": "2024-03-29T20:22:16.600914Z",
     "shell.execute_reply": "2024-03-29T20:22:16.600826Z",
     "shell.execute_reply.started": "2024-03-29T20:22:16.600816Z"
    }
   },
   "outputs": [],
   "source": [
    "#From the Literature Review, the ideal topics for LDA was found to be 7. \n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model = LdaModel(train_corpus, id2word=dictionary, num_topics=7, update_every=1, chunksize=10, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cee1400",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T20:22:16.602027Z",
     "iopub.status.idle": "2024-03-29T20:22:16.602237Z",
     "shell.execute_reply": "2024-03-29T20:22:16.602150Z",
     "shell.execute_reply.started": "2024-03-29T20:22:16.602140Z"
    }
   },
   "outputs": [],
   "source": [
    "#Evaluating LDA topic coherance values. \n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Convert preprocessed texts to PyTorch tensors and move to GPU if available\n",
    "preprocessed_texts_tensor = [torch.tensor(text) for text in preprocessed_texts]\n",
    "if torch.cuda.is_available():\n",
    "    preprocessed_texts_tensor = [text.to('cuda') for text in preprocessed_texts_tensor]\n",
    "\n",
    "# Calculate coherence values for each topic\n",
    "coherence_values = {}\n",
    "for topic_num in range(lda_model.num_topics):\n",
    "    topic_terms = lda_model.show_topic(topic_num)\n",
    "    topic_words = [term for term, _ in topic_terms]\n",
    "    coherence_model = CoherenceModel(topics=[topic_words], texts=preprocessed_texts_tensor, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_values[topic_num] = coherence_model.get_coherence()\n",
    "\n",
    "# Create a table of coherence values\n",
    "print(\"Topic\\tCoherence Value\")\n",
    "for topic_num, coherence_value in coherence_values.items():\n",
    "    print(f\"{topic_num}\\t{coherence_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bb1aa0",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T20:22:16.602905Z",
     "iopub.status.idle": "2024-03-29T20:22:16.603122Z",
     "shell.execute_reply": "2024-03-29T20:22:16.603016Z",
     "shell.execute_reply.started": "2024-03-29T20:22:16.603007Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert preprocessed texts to PyTorch tensors and move to GPU if available\n",
    "preprocessed_texts_tensor = [torch.tensor(text) for text in preprocessed_texts]\n",
    "if torch.cuda.is_available():\n",
    "    preprocessed_texts_tensor = [text.to('cuda') for text in preprocessed_texts_tensor]\n",
    "\n",
    "# Calculate coherence values for each topic\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=preprocessed_texts_tensor, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "print(\"Coherence Score for LDA model:\", coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e5688f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T20:22:16.604222Z",
     "iopub.status.idle": "2024-03-29T20:22:16.604423Z",
     "shell.execute_reply": "2024-03-29T20:22:16.604335Z",
     "shell.execute_reply.started": "2024-03-29T20:22:16.604325Z"
    }
   },
   "outputs": [],
   "source": [
    "#Evaluating HDP topic coherance values, which were found to have a range below as well as above the LDA model\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Get the top topics from HDP model\n",
    "hdp_topics = hdp_model.show_topics(num_topics=38, formatted=False)\n",
    "\n",
    "# Extract topic words for each topic\n",
    "topic_words = [[word for word, _ in topic] for topic_id, topic in hdp_topics]\n",
    "\n",
    "# Calculate coherence values for each topic\n",
    "coherence_values = {}\n",
    "for topic_num, words in enumerate(topic_words):\n",
    "    coherence_model = CoherenceModel(topics=[words], texts=preprocessed_texts_tensor, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_values[topic_num] = coherence_model.get_coherence()\n",
    "\n",
    "# Create a table of coherence values\n",
    "print(\"Topic\\tCoherence Value\")\n",
    "for topic_num, coherence_value in coherence_values.items():\n",
    "    print(f\"{topic_num}\\t{coherence_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148e2885",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T20:22:16.605379Z",
     "iopub.status.idle": "2024-03-29T20:22:16.605586Z",
     "shell.execute_reply": "2024-03-29T20:22:16.605498Z",
     "shell.execute_reply.started": "2024-03-29T20:22:16.605488Z"
    }
   },
   "outputs": [],
   "source": [
    "# Finding the overall coherhance value for HDP model\n",
    "hdp_topics = hdp_model.show_topics(num_topics=38, formatted=False)  # Get the top topics\n",
    "\n",
    "# Extract topic words for each topic\n",
    "topic_words = [[word for word, _ in topic] for topic_id, topic in hdp_topics]\n",
    "\n",
    "# Calculate coherence values for each topic\n",
    "coherence_values = {}\n",
    "for topic_num, words in enumerate(topic_words):\n",
    "    coherence_model = CoherenceModel(topics=[words], texts=preprocessed_texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_values[topic_num] = coherence_model.get_coherence()\n",
    "\n",
    "# Compute the average coherence value\n",
    "avg_coherence_value = sum(coherence_values.values()) / len(coherence_values)\n",
    "\n",
    "print(\"Overall Coherence Value for HDP model:\", avg_coherence_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f851d4b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T20:22:16.606463Z",
     "iopub.status.idle": "2024-03-29T20:22:16.606660Z",
     "shell.execute_reply": "2024-03-29T20:22:16.606575Z",
     "shell.execute_reply.started": "2024-03-29T20:22:16.606566Z"
    }
   },
   "outputs": [],
   "source": [
    "#Generate df with dominant topics, the topic contribution and topic keywords for LDA model\n",
    "import pandas as pd\n",
    "\n",
    "def format_topics(ldamodel=None, corpus=None, texts=None):\n",
    "    # Initialize an empty list to store rows\n",
    "    rows = []\n",
    "\n",
    "    # Iterate through each document in the corpus\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "\n",
    "        # Extract dominant topic, its contribution, and keywords\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # Dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                row_data = [int(topic_num), round(prop_topic, 4), topic_keywords, texts[i]]\n",
    "                rows.append(row_data)\n",
    "                break\n",
    "\n",
    "    # Create df\n",
    "    topics_df = pd.DataFrame(rows, columns=['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords', 'Text'])\n",
    "\n",
    "    return topics_df\n",
    "\n",
    "\n",
    "df_topic_keywords = format_topics(ldamodel=lda_model, corpus=corpus, texts=preprocessed_texts)\n",
    "\n",
    "df_topic_keywords.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75a88f6",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T20:22:16.607449Z",
     "iopub.status.idle": "2024-03-29T20:22:16.607642Z",
     "shell.execute_reply": "2024-03-29T20:22:16.607558Z",
     "shell.execute_reply.started": "2024-03-29T20:22:16.607547Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Generate df of represtative text for dominant topics for LDA model\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "topics_sorteddf_mallet = pd.DataFrame()\n",
    "topics_outdf_grpd = df_topic_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in topics_outdf_grpd:\n",
    "    topics_sorteddf_mallet = pd.concat([topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format DF\n",
    "topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
    "\n",
    "\n",
    "topics_sorteddf_mallet.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e230acf2",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T20:22:16.608592Z",
     "iopub.status.idle": "2024-03-29T20:22:16.608781Z",
     "shell.execute_reply": "2024-03-29T20:22:16.608700Z",
     "shell.execute_reply.started": "2024-03-29T20:22:16.608690Z"
    }
   },
   "outputs": [],
   "source": [
    "#Generate df with dominant topics, the topic contribution and topic keywords for HDP model\n",
    "import pandas as pd\n",
    "\n",
    "def topics_sentences(ldamodel=None, corpus=None, texts=None):\n",
    "    # Initialize an empty list to store rows\n",
    "    rows = []\n",
    "\n",
    "    # Iterate through each document in the corpus\n",
    "    for i, topics in enumerate(ldamodel[corpus]):\n",
    "        # Sort topics by contribution\n",
    "        topics = sorted(topics, key=lambda x: (x[1]), reverse=True)\n",
    "\n",
    "        # Extract dominant topic, its contribution, and keywords\n",
    "        for j, (topic_num, prop_topic) in enumerate(topics):\n",
    "            if j == 0:  # Dominant topic\n",
    "                topic_keywords = \", \".join([word for word, prop in ldamodel.show_topic(topic_num)])\n",
    "                row_data = [int(topic_num), round(prop_topic, 4), topic_keywords, texts[i]]\n",
    "                rows.append(row_data)\n",
    "                break\n",
    "\n",
    "    # Create df\n",
    "    topics_df = pd.DataFrame(rows, columns=['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords', 'Text'])\n",
    "\n",
    "    return topics_df\n",
    "\n",
    "df_topic_keywords = topics_sentences(ldamodel=hdp_model, corpus=corpus, texts=preprocessed_texts)\n",
    "\n",
    "df_topic_keywords.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0640fb",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T20:22:16.609770Z",
     "iopub.status.idle": "2024-03-29T20:22:16.609991Z",
     "shell.execute_reply": "2024-03-29T20:22:16.609899Z",
     "shell.execute_reply.started": "2024-03-29T20:22:16.609888Z"
    }
   },
   "outputs": [],
   "source": [
    "#Generate df of represtative text for dominant topics for HDP model\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "topics_sorteddf_mallet = pd.DataFrame()\n",
    "topics_outdf_grpd = df_topic_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in topics_outdf_grpd:\n",
    "    topics_sorteddf_mallet = pd.concat([topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
    "\n",
    "# Show\n",
    "topics_sorteddf_mallet.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6c101e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T20:22:16.610864Z",
     "iopub.status.idle": "2024-03-29T20:22:16.611095Z",
     "shell.execute_reply": "2024-03-29T20:22:16.610981Z",
     "shell.execute_reply.started": "2024-03-29T20:22:16.610970Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Ploting document word count against nubmer of documents for LDA model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=7, figsize=(20, 4))\n",
    "\n",
    "for i in range(7):\n",
    "    word_counts = []\n",
    "\n",
    "    \n",
    "    for doc in corpus:\n",
    "        # Get the topic distribution for the document\n",
    "        doc_topics = lda_model.get_document_topics(doc)\n",
    "\n",
    "        # Check if the current topic is the dominant topic for the document\n",
    "        for topic, prob in doc_topics:\n",
    "            if topic == i:\n",
    "                # Calculate the word count of the document and add it to the list\n",
    "                word_count = sum(count for _, count in doc)\n",
    "                word_counts.append(word_count)\n",
    "                break\n",
    "\n",
    "    axes[i].hist(word_counts, bins=30, alpha=0.5)\n",
    "    axes[i].set_title(f'Topic {i}')\n",
    "    axes[i].set_xlabel('Document Word Count')\n",
    "    axes[i].set_ylabel('Number of Documents')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06cb098",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T20:22:16.612039Z",
     "iopub.status.idle": "2024-03-29T20:22:16.612263Z",
     "shell.execute_reply": "2024-03-29T20:22:16.612171Z",
     "shell.execute_reply.started": "2024-03-29T20:22:16.612159Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Ploting document word count against nubmer of documents for HDP model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=5, figsize=(20, 12))\n",
    "\n",
    "\n",
    "for i in range(15):\n",
    "    word_counts = []\n",
    "\n",
    "    for doc in corpus:\n",
    "        doc_topics = hdp_model[doc]\n",
    "\n",
    "        for topic, prob in doc_topics:\n",
    "            if topic == i:\n",
    "                word_count = sum(count for _, count in doc)\n",
    "                word_counts.append(word_count)\n",
    "                break\n",
    "\n",
    "    # Determine the position of the subplot in the grid\n",
    "    row_index = i // 5\n",
    "    col_index = i % 5\n",
    "\n",
    "    axes[row_index, col_index].hist(word_counts, bins=30, alpha=0.5)\n",
    "    axes[row_index, col_index].set_title(f'Topic {i}')\n",
    "    axes[row_index, col_index].set_xlabel('Document Word Count')\n",
    "    axes[row_index, col_index].set_ylabel('Number of Documents')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5acec6",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T20:22:16.613133Z",
     "iopub.status.idle": "2024-03-29T20:22:16.613348Z",
     "shell.execute_reply": "2024-03-29T20:22:16.613259Z",
     "shell.execute_reply.started": "2024-03-29T20:22:16.613247Z"
    }
   },
   "outputs": [],
   "source": [
    "#Exploring the relevant terms for each topic of the LDA Model\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=lda_model.id2word)\n",
    "\n",
    "# Convert complex numbers to real numbers in topic coordinates\n",
    "vis.topic_coordinates['x'] = vis.topic_coordinates['x'].apply(lambda x: x.real)\n",
    "vis.topic_coordinates['y'] = vis.topic_coordinates['y'].apply(lambda y: y.real)\n",
    "\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1961c73c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T20:22:16.614503Z",
     "iopub.status.idle": "2024-03-29T20:22:16.614712Z",
     "shell.execute_reply": "2024-03-29T20:22:16.614624Z",
     "shell.execute_reply.started": "2024-03-29T20:22:16.614614Z"
    }
   },
   "outputs": [],
   "source": [
    "#Exploring the relevant terms for each topic of the HDP Model\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(hdp_model, corpus, dictionary=dictionary)\n",
    "\n",
    "# Convert complex numbers to real numbers in topic coordinates\n",
    "vis.topic_coordinates['x'] = vis.topic_coordinates['x'].apply(lambda x: x.real)\n",
    "vis.topic_coordinates['y'] = vis.topic_coordinates['y'].apply(lambda y: y.real)\n",
    "\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b434a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
