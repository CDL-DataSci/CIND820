{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0d4fbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cdlacey/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/cdlacey/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing relevant libraries\n",
    "import pdfminer\n",
    "from pdfminer.high_level import extract_text\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import download\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel\n",
    "import os\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pdfplumber\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import HdpModel\n",
    "import tensorflow\n",
    "import torch\n",
    "import concurrent.futures\n",
    "\n",
    "# Download other resources\n",
    "download('stopwords')\n",
    "download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2734122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a2d4c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files in datasource:  1972\n"
     ]
    }
   ],
   "source": [
    "#Initial stats - file count\n",
    "\n",
    "def count_files_in_folder(folder_path):\n",
    "    # Initialize a counter for files\n",
    "    file_count = 0\n",
    "\n",
    "    # Walk through the directory and count files\n",
    "    for _, _, files in os.walk(folder_path):\n",
    "        file_count += len(files)\n",
    "\n",
    "    return file_count\n",
    "\n",
    "folder_path = '/Users/cdlacey/TMU_DataScience/CIND820/Dataset_Full'\n",
    "total_files = count_files_in_folder(folder_path)\n",
    "print(\"Total files in datasource: \", total_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94d251db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages in all PDF files: 155385\n",
      "Mean page count per file: 78.8756345177665\n",
      "Median page count per file: 80.0\n"
     ]
    }
   ],
   "source": [
    "#Initial stats - page count\n",
    "\n",
    "def count_pages_and_stats(folder_path):\n",
    "    total_pages = 0\n",
    "    page_counts = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdf'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'rb') as file:\n",
    "                pdf_reader = PdfReader(file)\n",
    "                num_pages = len(pdf_reader.pages)\n",
    "                total_pages += num_pages\n",
    "                page_counts.append(num_pages)\n",
    "\n",
    "    mean_page_count = statistics.mean(page_counts)\n",
    "    median_page_count = statistics.median(page_counts)\n",
    "\n",
    "    return total_pages, mean_page_count, median_page_count\n",
    "\n",
    "folder_path = '/Users/cdlacey/TMU_DataScience/CIND820/Dataset_Full'\n",
    "total_pages, mean_page_count, median_page_count = count_pages_and_stats(folder_path)\n",
    "\n",
    "print(\"Total pages in all PDF files:\", total_pages)\n",
    "print(\"Mean page count per file:\", mean_page_count)\n",
    "print(\"Median page count per file:\", median_page_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "234aa441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-27 10:30:00.073160: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2024-03-27 10:30:00.073458: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-03-27 10:30:00.073473: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-03-27 10:30:00.073543: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-03-27 10:30:00.073639: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2024-03-27 10:30:00.092391: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-03-27 10:30:00.092406: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Function for preprocessing text\n",
    "def preprocess_text(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    french_stopwords = set(stopwords.words('french'))\n",
    "    stop_words.update(french_stopwords)\n",
    "    # Remove specific words or letters which are not useful\n",
    "    additional_stopwords = [\n",
    "        'mr.', 'mr', 'mrs.', 'ms.', 'speaker', 'bill', 'debate', 'hon', 'cpc', 'lib', 'bq', 'canadian', \n",
    "        'act', 'amend', 'amendment', 'canada', 'house', 'public', 'honour', 'minister', 'ministry', 'govern', \n",
    "        'member', 'program', 'primeminister', 'would', 'people', 'chair', 'committe', 'liber', 'polici', 'parliamentari', \n",
    "        'ndp', 'government', 'conserv', 'parties', 'partisan', 's', 'b', 'c', 'e', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'o', 'p',\n",
    "        'q', 'r', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'am', 'pm','year','time','motion','go', 'canadians', \n",
    "        'also', 'members', 'madam', 'committee', 'prime', 'senate', 'senator', 'hous',\n",
    "        'one', 'govern', 'liberal', 'conservative', 'liberals', 'conservatives', 'speech', 'parliamentarian',\n",
    "        'secretariat', 'ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'x', 'xi', '000', '1', '3', '5', '11', \n",
    "        '15', '22', '25', '2007', '2008', '2009', '2010', '2011',\n",
    "        '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023', '‚Äô',\n",
    "        '‚Äú', '‚Äù', \"’\",'...................',' ................................................',\n",
    "        '........',\"'s\"]\n",
    "    stop_words.update(additional_stopwords)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Remove numbers, symbols, and certain words\n",
    "    tokens = [re.sub(r'[^a-zA-Z]', '', token) for token in tokens]\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Directory path containing PDF files\n",
    "pdf_directory = '/Users/cdlacey/TMU_DataScience/CIND820/Dataset_Full'\n",
    "\n",
    "# List all PDF files in the directory\n",
    "pdf_files = [os.path.join(pdf_directory, file) for file in os.listdir(pdf_directory) if file.endswith('.pdf')]\n",
    "\n",
    "texts = []\n",
    "\n",
    "# Loop through each PDF file and extract text\n",
    "for pdf_file in pdf_files:\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "        texts.append(text)\n",
    "\n",
    "# Preprocess text\n",
    "preprocessed_texts = [preprocess_text(text) for text in texts]\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(preprocessed_texts)\n",
    "sequences = tokenizer.texts_to_sequences(preprocessed_texts)\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "maxlen = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "# Convert padded sequences to TensorFlow tensors\n",
    "corpus_tensor = tf.constant(padded_sequences)\n",
    "num_words = len(tokenizer.word_index) + 1  # Add 1 for padding token\n",
    "\n",
    "# Check if GPU is available\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    device = 'GPU'\n",
    "    print('Using GPU:', tf.test.gpu_device_name())\n",
    "else:\n",
    "    device = 'CPU'\n",
    "    print('Using CPU')\n",
    "\n",
    "# Perform computations using GPU-accelerated operations\n",
    "# Example: Apply some operation on corpus_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd108981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Corpus Size: 1576\n",
      "Testing Corpus Size: 394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 16:19:03.131377: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-03-29 16:19:03.251380: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Splitting data for cross-validation using TensorFlow\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Define the batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Check if the corpus has enough samples for splitting\n",
    "if len(corpus_tensor) == 0:\n",
    "    print(\"Error: The corpus is empty. Please provide a non-empty corpus.\")\n",
    "else:\n",
    "    # Split the corpus_tensor into train and test sets\n",
    "    train_corpus, test_corpus = train_test_split(corpus_tensor.numpy(), test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create datasets from train and test sets\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_corpus)\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices(test_corpus)\n",
    "    \n",
    "    # Print the number of samples in each set\n",
    "    print(\"Training Corpus Size:\", len(list(train_dataset)))\n",
    "    print(\"Testing Corpus Size:\", len(list(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df7c7aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 16:22:58.379890: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-03-29 16:22:58.379912: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dictionary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Train the HDP model\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[0;32m---> 17\u001b[0m     hdp_model \u001b[38;5;241m=\u001b[39m HdpModel(train_corpus_tensor, id2word\u001b[38;5;241m=\u001b[39mdictionary)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dictionary' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert train_corpus to TensorFlow tensor\n",
    "train_corpus_tensor = tf.convert_to_tensor(train_corpus, dtype=tf.float32)\n",
    "\n",
    "# Check if GPU is available\n",
    "if tf.config.experimental.list_physical_devices('GPU'):\n",
    "    device = '/GPU:0'\n",
    "    print('Using GPU:', tf.test.gpu_device_name())\n",
    "    # Move data to GPU\n",
    "    with tf.device(device):\n",
    "        train_corpus_tensor = train_corpus_tensor\n",
    "else:\n",
    "    device = '/CPU:0'\n",
    "    print('Using CPU')\n",
    "\n",
    "# Train the HDP model\n",
    "with tf.device(device):\n",
    "    hdp_model = HdpModel(train_corpus_tensor, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589839bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store document topics\n",
    "rows = []\n",
    "\n",
    "# Iterate through each document in the corpus\n",
    "for i, doc in enumerate(corpus_tensor):\n",
    "    # Convert doc to a numpy array\n",
    "    doc_np = doc.numpy()\n",
    "    # Evaluate document topics using HDP model\n",
    "    doc_topics = hdp_model(doc_np)\n",
    "    # Extract topic numbers and their probabilities\n",
    "    topic_numbers = [topic[0] for topic in doc_topics]\n",
    "    topic_probs = [topic[1] for topic in doc_topics]\n",
    "    # Append the document's topics to the rows list\n",
    "    rows.append([i, topic_numbers, topic_probs])\n",
    "\n",
    "# Create a DataFrame from the list of rows\n",
    "doc_topics_df = pd.DataFrame(rows, columns=['Document_Index', 'Topic_Numbers', 'Topic_Probabilities'])\n",
    "\n",
    "# Display the DataFrame\n",
    "doc_topics_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8c96c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an LDA model using the HDP model as a training mechanism\n",
    "lda_model_t = hdp_model.suggested_lda_model()\n",
    "\n",
    "# Initialize an empty set to store unique topics\n",
    "unique_topics = set()\n",
    "\n",
    "# Iterate through each document in the corpus\n",
    "for doc in corpus_tensor:\n",
    "    # Convert doc to a numpy array\n",
    "    doc_np = doc.numpy()\n",
    "    # Get the topic distribution for the document using the LDA model\n",
    "    doc_topics = lda_model_t.get_document_topics(doc_np)\n",
    "    # Update the set of unique topics\n",
    "    unique_topics.update([topic[0] for topic in doc_topics])\n",
    "\n",
    "# Count the number of unique topics identified\n",
    "num_topics_identified = len(unique_topics)\n",
    "print(f\"Number of topics identified by HDP model: {num_topics_identified}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2f2209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "# Define the number of topics\n",
    "num_topics = 7\n",
    "\n",
    "# Define the LDA model using TensorFlow Probability\n",
    "def create_lda_model(corpus_tensor, dictionary_tensor):\n",
    "    num_docs, num_terms = corpus_tensor.shape\n",
    "    lda = tfp.distributions.LDA(\n",
    "        topic_concentration=1.0 / num_topics,\n",
    "        doc_topic_prior=None,\n",
    "        topic_word_prior=None,\n",
    "        total_counts=None,\n",
    "        validate_args=False,\n",
    "        allow_nan_stats=True,\n",
    "        name='LDA'\n",
    "    )\n",
    "    return lda\n",
    "\n",
    "# Create the LDA model\n",
    "lda_model = create_lda_model(train_corpus_tensor, dictionary_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cee1400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic\tCoherence Value\n",
      "0\t0.6667405375023441\n",
      "1\t0.4336852003642694\n",
      "2\t0.36582764858671035\n",
      "3\t0.6667405375023441\n",
      "4\t0.6667405375023441\n",
      "5\t0.3906360018628302\n",
      "6\t0.6667405375023441\n"
     ]
    }
   ],
   "source": [
    "#Evaluating LDA topic coherance values. \n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Calculate coherence values for each topic\n",
    "coherence_values = {}\n",
    "for topic_num in range(lda_model.num_topics):\n",
    "    topic_terms = lda_model.show_topic(topic_num)\n",
    "    topic_words = [term for term, _ in topic_terms]\n",
    "    coherence_model = CoherenceModel(topics=[topic_words], texts=preprocessed_texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_values[topic_num] = coherence_model.get_coherence()\n",
    "\n",
    "# Create a table of coherence values\n",
    "print(\"Topic\\tCoherence Value\")\n",
    "for topic_num, coherence_value in coherence_values.items():\n",
    "    print(f\"{topic_num}\\t{coherence_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bb1aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate coherence for each topic\n",
    "def calculate_coherence(topic_words, preprocessed_texts, dictionary, coherence='c_v'):\n",
    "    coherence_values = {}\n",
    "    for topic_num, words in enumerate(topic_words):\n",
    "        topic_texts = [[word for word in text if word in words] for text in preprocessed_texts]\n",
    "        coherence_model = CoherenceModel(topics=[words], texts=topic_texts, dictionary=dictionary, coherence=coherence)\n",
    "        coherence_values[topic_num] = coherence_model.get_coherence()\n",
    "    return coherence_values\n",
    "\n",
    "# Extract topic words from the model\n",
    "topic_words = [[term for term, _ in lda_model.show_topic(topic_num)] for topic_num in range(num_topics)]\n",
    "\n",
    "# Calculate coherence values for each topic\n",
    "coherence_values = calculate_coherence(topic_words, preprocessed_texts, dictionary)\n",
    "\n",
    "# Compute the overall coherence score\n",
    "coherence_lda = np.mean(list(coherence_values.values()))\n",
    "\n",
    "print(\"Coherence Score for LDA model:\", coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e5688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate coherence for a single topic\n",
    "def calculate_coherence(topic_words, preprocessed_texts, dictionary, coherence='c_v'):\n",
    "    topic_texts = tf.ragged.constant([[word for word in text if word in words] for text in preprocessed_texts])\n",
    "    coherence_values = []\n",
    "    for topic_text in topic_texts:\n",
    "        coherence_model = CoherenceModel(topics=[topic_words], texts=[topic_text], dictionary=dictionary, coherence=coherence)\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "    return np.mean(coherence_values)\n",
    "\n",
    "# Extract topic words from the HDP model\n",
    "hdp_topics = hdp_model.show_topics(num_topics=35, formatted=False)\n",
    "topic_words = [[word for word, _ in topic] for topic_id, topic in hdp_topics]\n",
    "\n",
    "# Calculate coherence values for each topic\n",
    "coherence_values = {}\n",
    "for topic_num, words in enumerate(topic_words):\n",
    "    coherence_values[topic_num] = calculate_coherence(words, preprocessed_texts, dictionary)\n",
    "\n",
    "# Print coherence values\n",
    "print(\"Topic\\tCoherence Value\")\n",
    "for topic_num, coherence_value in coherence_values.items():\n",
    "    print(f\"{topic_num}\\t{coherence_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148e2885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate coherence for a single topic\n",
    "def calculate_coherence(topic_words, preprocessed_texts, dictionary, coherence='c_v'):\n",
    "    topic_texts = tf.ragged.constant([[word for word in text if word in words] for text in preprocessed_texts])\n",
    "    coherence_values = []\n",
    "    for topic_text in topic_texts:\n",
    "        coherence_model = CoherenceModel(topics=[topic_words], texts=[topic_text], dictionary=dictionary, coherence=coherence)\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "    return np.mean(coherence_values)\n",
    "\n",
    "# Extract topic words from the HDP model\n",
    "hdp_topics = hdp_model.show_topics(num_topics=35, formatted=False)\n",
    "topic_words = [[word for word, _ in topic] for topic_id, topic in hdp_topics]\n",
    "\n",
    "# Calculate coherence values for each topic\n",
    "coherence_values = {}\n",
    "for topic_num, words in enumerate(topic_words):\n",
    "    coherence_values[topic_num] = calculate_coherence(words, preprocessed_texts, dictionary)\n",
    "\n",
    "# Compute the average coherence value\n",
    "avg_coherence_value = sum(coherence_values.values()) / len(coherence_values)\n",
    "\n",
    "print(\"Overall Coherence Value for HDP model:\", avg_coherence_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f851d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate df with dominant topics, the topic contribution and topic keywords for LDA model\n",
    "def format_topics(ldamodel=None, corpus=None, texts=None):\n",
    "    # Initialize an empty list to store rows\n",
    "    rows = []\n",
    "\n",
    "    # Iterate through each document in the corpus\n",
    "    for i, row_list in enumerate(ldamodel(corpus)):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "\n",
    "        # Extract dominant topic, its contribution, and keywords\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # Dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                row_data = [int(topic_num), round(prop_topic, 4), topic_keywords, texts[i]]\n",
    "                rows.append(row_data)\n",
    "                break\n",
    "\n",
    "    # Create df\n",
    "    topics_df = pd.DataFrame(rows, columns=['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords', 'Text'])\n",
    "\n",
    "    return topics_df\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Generate df with dominant topics, the topic contribution and topic keywords for LDA model\n",
    "df_topic_keywords = format_topics(ldamodel=lda_model, corpus=corpus_tensor.numpy(), texts=preprocessed_texts)\n",
    "\n",
    "df_topic_keywords.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75a88f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set maximum column width for display\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "def generate_representative_text(df_topic_keywords):\n",
    "    # Group by dominant topic\n",
    "    topics_outdf_grpd = df_topic_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "    # Initialize an empty DataFrame to store representative texts for each topic\n",
    "    topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "    # Iterate over each group\n",
    "    for i, grp in topics_outdf_grpd:\n",
    "        # Sort the group by percentage contribution in descending order and select the top row\n",
    "        topics_sorteddf_mallet = pd.concat([topics_sorteddf_mallet, \n",
    "                                            grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
    "                                           axis=0)\n",
    "\n",
    "    # Reset index\n",
    "    topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Rename columns\n",
    "    topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
    "\n",
    "    return topics_sorteddf_mallet\n",
    "\n",
    "# Generate df of representative text for dominant topics for LDA model\n",
    "topics_sorteddf_mallet = generate_representative_text(df_topic_keywords)\n",
    "\n",
    "topics_sorteddf_mallet.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e230acf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate df with dominant topics, the topic contribution and topic keywords for HDP model\n",
    "import pandas as pd\n",
    "\n",
    "def generate_dominant_topics(ldamodel=None, corpus=None, texts=None):\n",
    "    # Initialize an empty list to store rows\n",
    "    rows = []\n",
    "\n",
    "    # Iterate through each document in the corpus\n",
    "    for i, topics in enumerate(ldamodel[corpus]):\n",
    "        # Sort topics by contribution\n",
    "        topics = sorted(topics, key=lambda x: (x[1]), reverse=True)\n",
    "\n",
    "        # Extract dominant topic, its contribution, and keywords\n",
    "        for j, (topic_num, prop_topic) in enumerate(topics):\n",
    "            if j == 0:  # Dominant topic\n",
    "                # Join topic keywords\n",
    "                topic_keywords = \", \".join([word for word, prop in ldamodel.show_topic(topic_num)])\n",
    "                # Append row data to the list of rows\n",
    "                row_data = [int(topic_num), round(prop_topic, 4), topic_keywords, texts[i]]\n",
    "                rows.append(row_data)\n",
    "                break\n",
    "\n",
    "    # Create DataFrame\n",
    "    topics_df = pd.DataFrame(rows, columns=['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords', 'Text'])\n",
    "\n",
    "    return topics_df\n",
    "\n",
    "# Generate DataFrame with dominant topics, the topic contribution, and topic keywords for HDP model\n",
    "df_topic_keywords = generate_dominant_topics(ldamodel=hdp_model, corpus=corpus, texts=preprocessed_texts)\n",
    "\n",
    "df_topic_keywords.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0640fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate df of represtative text for dominant topics for HDP model\n",
    "import pandas as pd\n",
    "\n",
    "# Set the maximum column width for display\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "# Group the DataFrame by 'Dominant_Topic' column\n",
    "topics_outdf_grpd = df_topic_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "# Initialize an empty list to store the selected rows\n",
    "selected_rows = []\n",
    "\n",
    "# Iterate over each group\n",
    "for _, grp in topics_outdf_grpd:\n",
    "    # Sort the group by 'Perc_Contribution' in descending order and select the top row\n",
    "    top_row = grp.sort_values('Perc_Contribution', ascending=False).head(1)\n",
    "    # Append the selected row to the list\n",
    "    selected_rows.append(top_row)\n",
    "\n",
    "# Concatenate the selected rows to create the final DataFrame\n",
    "topics_sorteddf_mallet = pd.concat(selected_rows, axis=0)\n",
    "\n",
    "# Reset the index\n",
    "topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Rename the columns\n",
    "topics_sorteddf_mallet.columns = ['Topic_Num', 'Topic_Perc_Contrib', 'Keywords', 'Representative Text']\n",
    "\n",
    "# Display the DataFrame\n",
    "topics_sorteddf_mallet.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6c101e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Ploting document word count against nubmer of documents for LDA model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=7, figsize=(20, 4))\n",
    "\n",
    "for i in range(7):\n",
    "    word_counts = []\n",
    "\n",
    "    for doc in corpus:\n",
    "        # Get the topic distribution for the document\n",
    "        doc_topics = lda_model.get_document_topics(doc)\n",
    "\n",
    "        # Check if the current topic is the dominant topic for the document\n",
    "        for topic, prob in doc_topics:\n",
    "            if topic == i:\n",
    "                # Calculate the word count of the document and add it to the list\n",
    "                word_count = sum(count for _, count in doc)\n",
    "                word_counts.append(word_count)\n",
    "                break\n",
    "\n",
    "    # Create a TensorFlow histogram\n",
    "    hist, bins = tf.histogram_fixed_width(word_counts, [0, max(word_counts)], nbins=30)\n",
    "\n",
    "    axes[i].bar(bins[:-1], hist, width=(bins[1]-bins[0]), alpha=0.5)\n",
    "    axes[i].set_title(f'Topic {i}')\n",
    "    axes[i].set_xlabel('Document Word Count')\n",
    "    axes[i].set_ylabel('Number of Documents')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06cb098",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Ploting document word count against nubmer of documents for HDP model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=5, figsize=(20, 12))\n",
    "\n",
    "for i in range(15):\n",
    "    word_counts = []\n",
    "\n",
    "    for doc in corpus:\n",
    "        doc_topics = hdp_model[doc]\n",
    "\n",
    "        for topic, prob in doc_topics:\n",
    "            if topic == i:\n",
    "                word_count = sum(count for _, count in doc)\n",
    "                word_counts.append(word_count)\n",
    "                break\n",
    "\n",
    "    # Determine the position of the subplot in the grid\n",
    "    row_index = i // 5\n",
    "    col_index = i % 5\n",
    "\n",
    "    # Create a TensorFlow histogram\n",
    "    hist, bins = tf.histogram_fixed_width(word_counts, [0, max(word_counts)], nbins=30)\n",
    "\n",
    "    axes[row_index, col_index].bar(bins[:-1], hist, width=(bins[1]-bins[0]), alpha=0.5)\n",
    "    axes[row_index, col_index].set_title(f'Topic {i}')\n",
    "    axes[row_index, col_index].set_xlabel('Document Word Count')\n",
    "    axes[row_index, col_index].set_ylabel('Number of Documents')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5acec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Extract topic coordinates\n",
    "topic_coordinates = vis.topic_coordinates\n",
    "\n",
    "# Convert complex numbers to real numbers in topic coordinates\n",
    "topic_coordinates['x'] = topic_coordinates['x'].apply(lambda x: x.real)\n",
    "topic_coordinates['y'] = topic_coordinates['y'].apply(lambda y: y.real)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=topic_coordinates, x='x', y='y', hue='topics', palette='tab10', legend='full', s=200)\n",
    "plt.title('Topic Visualization')\n",
    "plt.xlabel('X-coordinate')\n",
    "plt.ylabel('Y-coordinate')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1961c73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Extract topic coordinates\n",
    "topic_coordinates = vis.topic_coordinates\n",
    "\n",
    "# Convert complex numbers to real numbers in topic coordinates\n",
    "topic_coordinates['x'] = topic_coordinates['x'].apply(lambda x: x.real)\n",
    "topic_coordinates['y'] = topic_coordinates['y'].apply(lambda y: y.real)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=topic_coordinates, x='x', y='y', hue='topics', palette='tab10', legend='full', s=200)\n",
    "plt.title('Topic Visualization')\n",
    "plt.xlabel('X-coordinate')\n",
    "plt.ylabel('Y-coordinate')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b434a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
