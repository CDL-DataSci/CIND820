{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fadcc6c-c439-42ed-9780-bf1aead06cb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T05:59:26.879408Z",
     "iopub.status.busy": "2024-04-01T05:59:26.878948Z",
     "iopub.status.idle": "2024-04-01T05:59:29.406776Z",
     "shell.execute_reply": "2024-04-01T05:59:29.406123Z",
     "shell.execute_reply.started": "2024-04-01T05:59:26.879386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfminer in /usr/local/lib/python3.9/dist-packages (20191125)\n",
      "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.9/dist-packages (20231228)\n",
      "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.9/dist-packages (3.0.1)\n",
      "Requirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (4.3.2)\n",
      "Requirement already satisfied: wordcloud in /usr/local/lib/python3.9/dist-packages (1.9.3)\n",
      "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.9/dist-packages (0.11.0)\n",
      "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.9/dist-packages (from pdfminer) (3.20.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.9/dist-packages (from pdfminer.six) (42.0.5)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from pdfminer.six) (2.1.1)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in /usr/local/lib/python3.9/dist-packages (from PyPDF2) (4.4.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.9.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.23.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from wordcloud) (3.6.1)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from wordcloud) (9.2.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.9/dist-packages (from pdfplumber) (4.28.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.9/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.15.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (23.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (4.38.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (1.0.7)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfminer pdfminer.six PyPDF2 gensim wordcloud pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0d4fbf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T05:59:29.408241Z",
     "iopub.status.busy": "2024-04-01T05:59:29.408042Z",
     "iopub.status.idle": "2024-04-01T05:59:34.271597Z",
     "shell.execute_reply": "2024-04-01T05:59:34.270984Z",
     "shell.execute_reply.started": "2024-04-01T05:59:29.408222Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing relevant libraries\n",
    "from pdfminer.high_level import extract_text\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import download\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel\n",
    "import os\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pdfplumber\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import HdpModel\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "# Download other resources\n",
    "download('stopwords')\n",
    "download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a2d4c38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T05:59:34.272785Z",
     "iopub.status.busy": "2024-04-01T05:59:34.272399Z",
     "iopub.status.idle": "2024-04-01T05:59:34.278776Z",
     "shell.execute_reply": "2024-04-01T05:59:34.278228Z",
     "shell.execute_reply.started": "2024-04-01T05:59:34.272766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files in datasource:  1971\n"
     ]
    }
   ],
   "source": [
    "#Initial stats - file count\n",
    "\n",
    "def count_files_in_folder(folder_path):\n",
    "    # Initialize a counter for files\n",
    "    file_count = 0\n",
    "\n",
    "    # Walk through the directory and count files\n",
    "    for _, _, files in os.walk(folder_path):\n",
    "        file_count += len(files)\n",
    "\n",
    "    return file_count\n",
    "\n",
    "folder_path = '/notebooks/CIND820/Datasource'\n",
    "total_files = count_files_in_folder(folder_path)\n",
    "print(\"Total files in datasource: \", total_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94d251db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T05:59:34.280360Z",
     "iopub.status.busy": "2024-04-01T05:59:34.280192Z",
     "iopub.status.idle": "2024-04-01T06:00:16.598440Z",
     "shell.execute_reply": "2024-04-01T06:00:16.597921Z",
     "shell.execute_reply.started": "2024-04-01T05:59:34.280346Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages in all PDF files: 155385\n",
      "Mean page count per file: 78.8756345177665\n",
      "Median page count per file: 80.0\n"
     ]
    }
   ],
   "source": [
    "#Initial stats - page count\n",
    "\n",
    "def count_pages_and_stats(folder_path):\n",
    "    total_pages = 0\n",
    "    page_counts = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdf'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'rb') as file:\n",
    "                pdf_reader = PdfReader(file)\n",
    "                num_pages = len(pdf_reader.pages)\n",
    "                total_pages += num_pages\n",
    "                page_counts.append(num_pages)\n",
    "\n",
    "    mean_page_count = statistics.mean(page_counts)\n",
    "    median_page_count = statistics.median(page_counts)\n",
    "\n",
    "    return total_pages, mean_page_count, median_page_count\n",
    "\n",
    "folder_path = '/notebooks/CIND820/Datasource'\n",
    "total_pages, mean_page_count, median_page_count = count_pages_and_stats(folder_path)\n",
    "\n",
    "print(\"Total pages in all PDF files:\", total_pages)\n",
    "print(\"Mean page count per file:\", mean_page_count)\n",
    "print(\"Median page count per file:\", median_page_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "234aa441",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T06:00:16.599870Z",
     "iopub.status.busy": "2024-04-01T06:00:16.599175Z",
     "iopub.status.idle": "2024-04-01T12:26:38.125012Z",
     "shell.execute_reply": "2024-04-01T12:26:38.124033Z",
     "shell.execute_reply.started": "2024-04-01T06:00:16.599849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6h 23min 39s, sys: 1min 41s, total: 6h 25min 20s\n",
      "Wall time: 6h 26min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Function for preprocessing text\n",
    "def preprocess_text(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    french_stopwords = set(stopwords.words('french'))\n",
    "    stop_words.update(french_stopwords)\n",
    "    # Remove specific words or letters which are not useful\n",
    "    additional_stopwords = [\n",
    "        'mr.', 'mr', 'mrs.', 'ms.', 'speaker', 'bill', 'debate', 'hon', 'cpc', 'lib', 'bq', 'canadian', \n",
    "        'act', 'amend', 'amendment', 'canada', 'house', 'public', 'honour', 'minister', 'ministry', 'govern', \n",
    "        'member', 'program', 'primeminister', 'would', 'people', 'chair', 'committe', 'liber', 'polici', 'parliamentari', \n",
    "        'ndp', 'government', 'conserv', 'parties', 'partisan', 's', 'b', 'c', 'e', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'o', 'p',\n",
    "        'q', 'r', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'am', 'pm','year','time','motion','go', 'canadians', \n",
    "        'also', 'members', 'madam', 'committee', 'prime', 'senate', 'senator', 'hous',\n",
    "        'one', 'govern', 'liberal', 'conservative', 'liberals', 'conservatives', 'speech', 'parliamentarian',\n",
    "        'secretariat', 'ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'x', 'xi', '000', '1', '3', '5', '11', \n",
    "        '15', '22', '25', '2007', '2008', '2009', '2010', '2011',\n",
    "        '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023', '‚Äô',\n",
    "        '‚Äú', '‚Äù', \"’\",'...................',' ................................................',\n",
    "        '........',\"'s\"]\n",
    "    stop_words.update(additional_stopwords)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Remove numbers, symbols, and certain words\n",
    "    tokens = [re.sub(r'[^a-zA-Z]', '', token) for token in tokens]\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Directory path containing PDF files\n",
    "pdf_directory = '/notebooks/CIND820/Datasource'\n",
    "\n",
    "# List all PDF files in the directory\n",
    "pdf_files = [os.path.join(pdf_directory, file) for file in os.listdir(pdf_directory) if file.endswith('.pdf')]\n",
    "\n",
    "texts = []\n",
    "\n",
    "# Loop through each PDF file and extract text\n",
    "for pdf_file in pdf_files:\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "        texts.append(text)\n",
    "\n",
    "# Preprocess text\n",
    "preprocessed_texts = [preprocess_text(text) for text in texts]\n",
    "\n",
    "# Create a dictionary from the preprocessed text\n",
    "dictionary = Dictionary(preprocessed_texts)\n",
    "\n",
    "# Create a corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in preprocessed_texts]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd108981",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T12:26:38.126185Z",
     "iopub.status.busy": "2024-04-01T12:26:38.126003Z",
     "iopub.status.idle": "2024-04-01T12:26:38.131085Z",
     "shell.execute_reply": "2024-04-01T12:26:38.130483Z",
     "shell.execute_reply.started": "2024-04-01T12:26:38.126168Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.33 ms, sys: 0 ns, total: 1.33 ms\n",
      "Wall time: 1.27 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#spliting data for cross validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_corpus, test_corpus = train_test_split(corpus, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df7c7aa2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T12:26:38.132361Z",
     "iopub.status.busy": "2024-04-01T12:26:38.132005Z",
     "iopub.status.idle": "2024-04-01T12:42:35.299394Z",
     "shell.execute_reply": "2024-04-01T12:42:35.298947Z",
     "shell.execute_reply.started": "2024-04-01T12:26:38.132343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50min 53s, sys: 57min 9s, total: 1h 48min 3s\n",
      "Wall time: 15min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train the HDP model\n",
    "hdp_model = HdpModel(train_corpus, id2word=dictionary, max_chunks=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "589839bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T12:42:35.300578Z",
     "iopub.status.busy": "2024-04-01T12:42:35.300063Z",
     "iopub.status.idle": "2024-04-01T12:45:04.851124Z",
     "shell.execute_reply": "2024-04-01T12:45:04.844110Z",
     "shell.execute_reply.started": "2024-04-01T12:42:35.300561Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 2s, sys: 10min 49s, total: 23min 51s\n",
      "Wall time: 2min 29s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_Index</th>\n",
       "      <th>Topic_Numbers</th>\n",
       "      <th>Topic_Probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 6, 25]</td>\n",
       "      <td>[0.8253534430054159, 0.077472882922487, 0.0178...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[0.9999509368079036]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[0.22171735455962982, 0.7723314528917588]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0.9999800145261954]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[0.9999645659240654]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[0.9641942180665692, 0.021668186716909368]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[0.9999580695744752]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>[0, 3, 25]</td>\n",
       "      <td>[0.42904754761298824, 0.5368120904503535, 0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>[0, 1, 31]</td>\n",
       "      <td>[0.8170735246497098, 0.07354199502906827, 0.10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0.9900797852570569]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_Index  Topic_Numbers  \\\n",
       "0               0  [0, 1, 6, 25]   \n",
       "1               1            [1]   \n",
       "2               2         [0, 1]   \n",
       "3               3            [0]   \n",
       "4               4            [1]   \n",
       "5               5         [0, 1]   \n",
       "6               6            [1]   \n",
       "7               7     [0, 3, 25]   \n",
       "8               8     [0, 1, 31]   \n",
       "9               9            [0]   \n",
       "\n",
       "                                 Topic_Probabilities  \n",
       "0  [0.8253534430054159, 0.077472882922487, 0.0178...  \n",
       "1                               [0.9999509368079036]  \n",
       "2          [0.22171735455962982, 0.7723314528917588]  \n",
       "3                               [0.9999800145261954]  \n",
       "4                               [0.9999645659240654]  \n",
       "5         [0.9641942180665692, 0.021668186716909368]  \n",
       "6                               [0.9999580695744752]  \n",
       "7  [0.42904754761298824, 0.5368120904503535, 0.03...  \n",
       "8  [0.8170735246497098, 0.07354199502906827, 0.10...  \n",
       "9                               [0.9900797852570569]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#First evaluation of HDP model and number of topics identified per document)\n",
    "rows = []\n",
    "\n",
    "# Iterate through each document in the corpus\n",
    "for i, doc in enumerate(corpus):\n",
    "    doc_topics = hdp_model[doc]\n",
    "    # Extract topic numbers and their probabilities\n",
    "    topic_numbers = [topic[0] for topic in doc_topics]\n",
    "    topic_probs = [topic[1] for topic in doc_topics]\n",
    "    # Append the document's topics to the rows list\n",
    "    rows.append([i, topic_numbers, topic_probs])\n",
    "\n",
    "# Create a DataFrame from the list of rows\n",
    "doc_topics_df = pd.DataFrame(rows, columns=['Document_Index', 'Topic_Numbers', 'Topic_Probabilities'])\n",
    "\n",
    "# Display the DataFrame\n",
    "doc_topics_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc8c96c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T12:45:04.852009Z",
     "iopub.status.busy": "2024-04-01T12:45:04.851821Z",
     "iopub.status.idle": "2024-04-01T12:46:55.157790Z",
     "shell.execute_reply": "2024-04-01T12:46:55.157064Z",
     "shell.execute_reply.started": "2024-04-01T12:45:04.851993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics identified by HDP model: 44\n",
      "CPU times: user 7min 38s, sys: 6min 40s, total: 14min 18s\n",
      "Wall time: 1min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# To find an approximate number of total topics identified within the HDP model, I found it easiest to train an\n",
    "#LDA model on the HDP model. \n",
    "# Here we'll train an LDA model using the HDP model as a training mechanism\n",
    "lda_model_t = hdp_model.suggested_lda_model()\n",
    "\n",
    "# Get the topic distributions for each document\n",
    "doc_topics = [lda_model_t.get_document_topics(doc) for doc in corpus]\n",
    "\n",
    "# Count the number of unique topics\n",
    "unique_topics = set()\n",
    "for doc_topics in doc_topics:\n",
    "    unique_topics.update([topic[0] for topic in doc_topics])\n",
    "\n",
    "num_topics_identified = len(unique_topics)\n",
    "print(f\"Number of topics identified by HDP model: {num_topics_identified}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e2f2209",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T12:46:55.160635Z",
     "iopub.status.busy": "2024-04-01T12:46:55.160432Z",
     "iopub.status.idle": "2024-04-01T13:44:06.077280Z",
     "shell.execute_reply": "2024-04-01T13:44:06.076736Z",
     "shell.execute_reply.started": "2024-04-01T12:46:55.160617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 17min 7s, sys: 35min 38s, total: 1h 52min 46s\n",
      "Wall time: 57min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Crossvalidation using an n-fold analysis showed that the ideal number of topics for LDA was 7. \n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model = LdaModel(train_corpus, id2word=dictionary, num_topics=7, update_every=1, chunksize=10, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3cee1400",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T17:38:47.319849Z",
     "iopub.status.busy": "2024-04-01T17:38:47.319431Z",
     "iopub.status.idle": "2024-04-01T17:48:07.956677Z",
     "shell.execute_reply": "2024-04-01T17:48:07.955728Z",
     "shell.execute_reply.started": "2024-04-01T17:38:47.319827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic\tCoherence Value\n",
      "0\t0.2877520946346136\n",
      "1\t0.4072832441954907\n",
      "2\t0.2877520946346136\n",
      "3\t0.2877520946346136\n",
      "4\t0.2877520946346136\n",
      "5\t0.5201127737203777\n",
      "6\t0.2877520946346136\n",
      "CPU times: user 5min 21s, sys: 2min 24s, total: 7min 46s\n",
      "Wall time: 9min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Evaluating LDA topic coherance values. \n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Calculate coherence values for each topic\n",
    "coherence_values = {}\n",
    "for topic_num in range(lda_model.num_topics):\n",
    "    topic_terms = lda_model.show_topic(topic_num)\n",
    "    topic_words = [term for term, _ in topic_terms]\n",
    "    coherence_model = CoherenceModel(topics=[topic_words], texts=preprocessed_texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_values[topic_num] = coherence_model.get_coherence()\n",
    "\n",
    "# Create a table of coherence values\n",
    "print(\"Topic\\tCoherence Value\")\n",
    "for topic_num, coherence_value in coherence_values.items():\n",
    "    print(f\"{topic_num}\\t{coherence_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1bb1aa0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T17:48:07.958396Z",
     "iopub.status.busy": "2024-04-01T17:48:07.958208Z",
     "iopub.status.idle": "2024-04-01T17:49:51.727006Z",
     "shell.execute_reply": "2024-04-01T17:49:51.726037Z",
     "shell.execute_reply.started": "2024-04-01T17:48:07.958376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score for LDA model: 0.4838705994123883\n",
      "CPU times: user 47.8 s, sys: 20.6 s, total: 1min 8s\n",
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Finding the overall LDA model coherance value\n",
    "topics = lda_model.show_topics(num_topics=-1, formatted=False)\n",
    "\n",
    "# Calculate coherence values for each topic\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=preprocessed_texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "print(\"Coherence Score for LDA model:\", coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63e5688f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T17:49:51.728263Z",
     "iopub.status.busy": "2024-04-01T17:49:51.728054Z",
     "iopub.status.idle": "2024-04-01T19:02:48.554047Z",
     "shell.execute_reply": "2024-04-01T19:02:48.553529Z",
     "shell.execute_reply.started": "2024-04-01T17:49:51.728243Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic\tCoherence Value\n",
      "0\t0.31097972213760794\n",
      "1\t0.36657245798384186\n",
      "2\t0.36379747052706735\n",
      "3\t0.2961260355057365\n",
      "4\t0.2572518690754245\n",
      "5\t0.3475858236367159\n",
      "6\t0.38161437100159673\n",
      "7\t0.3347228150652374\n",
      "8\t0.3078476751753149\n",
      "9\t0.3439878829171348\n",
      "10\t0.3302024879942712\n",
      "11\t0.30946572114611176\n",
      "12\t0.26264561701793576\n",
      "13\t0.35808041806000857\n",
      "14\t0.3178612322881995\n",
      "15\t0.25012210570691595\n",
      "16\t0.31915602490604644\n",
      "17\t0.3589114421519376\n",
      "18\t0.34744533528969324\n",
      "19\t0.34093999918071\n",
      "20\t0.30094768084867873\n",
      "21\t0.31758213653333833\n",
      "22\t0.29554883934558934\n",
      "23\t0.30463450821015764\n",
      "24\t0.3344966630888714\n",
      "25\t0.3547754280871431\n",
      "26\t0.4088582699835227\n",
      "27\t0.3462581512768639\n",
      "28\t0.41740185638713967\n",
      "29\t0.27305484454884477\n",
      "30\t0.2173206127948147\n",
      "31\t0.3335069187240501\n",
      "32\t0.2416487785530701\n",
      "33\t0.28455495856646973\n",
      "34\t0.2900785597990573\n",
      "35\t0.31303794587052575\n",
      "36\t0.2692111079049069\n",
      "37\t0.29658764258124537\n",
      "38\t0.2882780561799173\n",
      "39\t0.3495894802818034\n",
      "40\t0.34327623880891295\n",
      "41\t0.3049280125428918\n",
      "CPU times: user 36min 8s, sys: 14min 42s, total: 50min 50s\n",
      "Wall time: 1h 12min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Evaluating HDP topic coherance values, which were found to have a range below as well as above the LDA model\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "hdp_topics = hdp_model.show_topics(num_topics=42, formatted=False)  # Get the top topics\n",
    "\n",
    "# Extract topic words for each topic\n",
    "topic_words = [[word for word, _ in topic] for topic_id, topic in hdp_topics]\n",
    "\n",
    "# Calculate coherence values for each topic\n",
    "coherence_values = {}\n",
    "for topic_num, words in enumerate(topic_words):\n",
    "    coherence_model = CoherenceModel(topics=[words], texts=preprocessed_texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_values[topic_num] = coherence_model.get_coherence()\n",
    "\n",
    "# Create a table of coherence values\n",
    "print(\"Topic\\tCoherence Value\")\n",
    "for topic_num, coherence_value in coherence_values.items():\n",
    "    print(f\"{topic_num}\\t{coherence_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "148e2885",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T19:02:48.556447Z",
     "iopub.status.busy": "2024-04-01T19:02:48.555662Z",
     "iopub.status.idle": "2024-04-01T20:15:22.371952Z",
     "shell.execute_reply": "2024-04-01T20:15:22.371000Z",
     "shell.execute_reply.started": "2024-04-01T19:02:48.556421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Coherence Value for HDP model: 0.31883079042107915\n",
      "CPU times: user 35min 49s, sys: 14min 36s, total: 50min 26s\n",
      "Wall time: 1h 12min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Finding the overall coherhance value for HDP model\n",
    "hdp_topics = hdp_model.show_topics(num_topics=42, formatted=False)  # Get the top topics\n",
    "\n",
    "# Extract topic words for each topic\n",
    "topic_words = [[word for word, _ in topic] for topic_id, topic in hdp_topics]\n",
    "\n",
    "# Calculate coherence values for each topic\n",
    "coherence_values = {}\n",
    "for topic_num, words in enumerate(topic_words):\n",
    "    coherence_model = CoherenceModel(topics=[words], texts=preprocessed_texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_values[topic_num] = coherence_model.get_coherence()\n",
    "\n",
    "# Compute the average coherence value\n",
    "avg_coherence_value = sum(coherence_values.values()) / len(coherence_values)\n",
    "\n",
    "print(\"Overall Coherence Value for HDP model:\", avg_coherence_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f851d4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T20:15:22.373978Z",
     "iopub.status.busy": "2024-04-01T20:15:22.373247Z",
     "iopub.status.idle": "2024-04-01T20:18:08.922845Z",
     "shell.execute_reply": "2024-04-01T20:18:08.921967Z",
     "shell.execute_reply.started": "2024-04-01T20:15:22.373952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 19s, sys: 18min 34s, total: 33min 53s\n",
      "Wall time: 2min 46s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Perc_Contribution</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>work, need, support, make, want, like, right, know, countri, english</td>\n",
       "      <td>[common, debat, volum, number, session, parliament, offici, report, hansard, tuesday, octob, hon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>work, need, support, make, want, like, right, know, countri, english</td>\n",
       "      <td>[parliament, session, common, debat, offici, report, hansard, volum, thursday, octob, honour, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>work, need, support, make, want, like, right, know, countri, english</td>\n",
       "      <td>[common, debat, volum, number, session, parliament, offici, report, hansard, monday, februari, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9976</td>\n",
       "      <td>work, need, support, make, want, like, right, know, countri, english</td>\n",
       "      <td>[common, debat, volum, number, session, parliament, offici, report, hansard, thursday, march, ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>work, need, support, make, want, like, right, know, countri, english</td>\n",
       "      <td>[parliament, session, common, debat, offici, report, hansard, volum, tuesday, may, honour, antho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>work, need, support, make, want, like, right, know, countri, english</td>\n",
       "      <td>[common, debat, volum, number, session, parliament, offici, report, hansard, friday, may, honour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>work, need, support, make, want, like, right, know, countri, english</td>\n",
       "      <td>[parliament, session, common, debat, offici, report, hansard, volum, thursday, decemb, honour, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>work, need, support, make, want, like, right, know, countri, english</td>\n",
       "      <td>[common, debat, volum, number, session, parliament, offici, report, hansard, wednesday, june, ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>work, need, support, make, want, like, right, know, countri, english</td>\n",
       "      <td>[common, debat, volum, number, session, parliament, offici, report, hansard, thursday, novemb, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9723</td>\n",
       "      <td>work, need, support, make, want, like, right, know, countri, english</td>\n",
       "      <td>[common, debat, volum, number, session, parliament, offici, report, hansard, tuesday, june, hono...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dominant_Topic  Perc_Contribution  \\\n",
       "0               1             0.9997   \n",
       "1               1             1.0000   \n",
       "2               1             0.9978   \n",
       "3               1             0.9976   \n",
       "4               1             1.0000   \n",
       "5               1             0.9999   \n",
       "6               1             1.0000   \n",
       "7               1             0.9970   \n",
       "8               1             0.9999   \n",
       "9               1             0.9723   \n",
       "\n",
       "                                                         Topic_Keywords  \\\n",
       "0  work, need, support, make, want, like, right, know, countri, english   \n",
       "1  work, need, support, make, want, like, right, know, countri, english   \n",
       "2  work, need, support, make, want, like, right, know, countri, english   \n",
       "3  work, need, support, make, want, like, right, know, countri, english   \n",
       "4  work, need, support, make, want, like, right, know, countri, english   \n",
       "5  work, need, support, make, want, like, right, know, countri, english   \n",
       "6  work, need, support, make, want, like, right, know, countri, english   \n",
       "7  work, need, support, make, want, like, right, know, countri, english   \n",
       "8  work, need, support, make, want, like, right, know, countri, english   \n",
       "9  work, need, support, make, want, like, right, know, countri, english   \n",
       "\n",
       "                                                                                                  Text  \n",
       "0  [common, debat, volum, number, session, parliament, offici, report, hansard, tuesday, octob, hon...  \n",
       "1  [parliament, session, common, debat, offici, report, hansard, volum, thursday, octob, honour, an...  \n",
       "2  [common, debat, volum, number, session, parliament, offici, report, hansard, monday, februari, h...  \n",
       "3  [common, debat, volum, number, session, parliament, offici, report, hansard, thursday, march, ho...  \n",
       "4  [parliament, session, common, debat, offici, report, hansard, volum, tuesday, may, honour, antho...  \n",
       "5  [common, debat, volum, number, session, parliament, offici, report, hansard, friday, may, honour...  \n",
       "6  [parliament, session, common, debat, offici, report, hansard, volum, thursday, decemb, honour, a...  \n",
       "7  [common, debat, volum, number, session, parliament, offici, report, hansard, wednesday, june, ho...  \n",
       "8  [common, debat, volum, number, session, parliament, offici, report, hansard, thursday, novemb, h...  \n",
       "9  [common, debat, volum, number, session, parliament, offici, report, hansard, tuesday, june, hono...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#Generate df with dominant topics, the topic contribution and topic keywords for LDA model\n",
    "import pandas as pd\n",
    "\n",
    "def format_topics(ldamodel=None, corpus=None, texts=None):\n",
    "    # Initialize an empty list to store rows\n",
    "    rows = []\n",
    "\n",
    "    # Iterate through each document in the corpus\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "\n",
    "        # Extract dominant topic, its contribution, and keywords\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # Dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                row_data = [int(topic_num), round(prop_topic, 4), topic_keywords, texts[i]]\n",
    "                rows.append(row_data)\n",
    "                break\n",
    "\n",
    "    # Create df\n",
    "    topics_df = pd.DataFrame(rows, columns=['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords', 'Text'])\n",
    "\n",
    "    return topics_df\n",
    "\n",
    "\n",
    "df_topic_keywords = format_topics(ldamodel=lda_model, corpus=corpus, texts=preprocessed_texts)\n",
    "\n",
    "df_topic_keywords.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f75a88f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T20:18:08.924184Z",
     "iopub.status.busy": "2024-04-01T20:18:08.923867Z",
     "iopub.status.idle": "2024-04-01T20:18:08.941389Z",
     "shell.execute_reply": "2024-04-01T20:18:08.940766Z",
     "shell.execute_reply.started": "2024-04-01T20:18:08.924161Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.34 ms, sys: 3.84 ms, total: 7.18 ms\n",
      "Wall time: 7.21 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Num</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Representative Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>work, need, support, make, want, like, right, know, countri, english</td>\n",
       "      <td>[common, debat, volum, number, session, parliament, offici, report, hansard, tuesday, octob, hon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.8595</td>\n",
       "      <td>ontario, qubec, britishcolumbia, hawn, lukiwski, theminist, tom, associ, alberta, deanallison</td>\n",
       "      <td>[common, debat, volum, number, session, parliament, offici, report, hansard, friday, april, hono...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic_Num  Topic_Perc_Contrib  \\\n",
       "0          1              1.0000   \n",
       "1          5              0.8595   \n",
       "\n",
       "                                                                                        Keywords  \\\n",
       "0                           work, need, support, make, want, like, right, know, countri, english   \n",
       "1  ontario, qubec, britishcolumbia, hawn, lukiwski, theminist, tom, associ, alberta, deanallison   \n",
       "\n",
       "                                                                                   Representative Text  \n",
       "0  [common, debat, volum, number, session, parliament, offici, report, hansard, tuesday, octob, hon...  \n",
       "1  [common, debat, volum, number, session, parliament, offici, report, hansard, friday, april, hono...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#Generate df of represtative text for dominant topics for LDA model\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "topics_sorteddf_mallet = pd.DataFrame()\n",
    "topics_outdf_grpd = df_topic_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in topics_outdf_grpd:\n",
    "    topics_sorteddf_mallet = pd.concat([topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format DF\n",
    "topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
    "\n",
    "\n",
    "topics_sorteddf_mallet.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e230acf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T20:18:08.942420Z",
     "iopub.status.busy": "2024-04-01T20:18:08.942223Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Generate df with dominant topics, the topic contribution and topic keywords for HDP model\n",
    "import pandas as pd\n",
    "\n",
    "def topics_sentences(ldamodel=None, corpus=None, texts=None):\n",
    "    # Initialize an empty list to store rows\n",
    "    rows = []\n",
    "\n",
    "    # Iterate through each document in the corpus\n",
    "    for i, topics in enumerate(ldamodel[corpus]):\n",
    "        # Sort topics by contribution\n",
    "        topics = sorted(topics, key=lambda x: (x[1]), reverse=True)\n",
    "\n",
    "        # Extract dominant topic, its contribution, and keywords\n",
    "        for j, (topic_num, prop_topic) in enumerate(topics):\n",
    "            if j == 0:  # Dominant topic\n",
    "                topic_keywords = \", \".join([word for word, prop in ldamodel.show_topic(topic_num)])\n",
    "                row_data = [int(topic_num), round(prop_topic, 4), topic_keywords, texts[i]]\n",
    "                rows.append(row_data)\n",
    "                break\n",
    "\n",
    "    # Create df\n",
    "    topics_df = pd.DataFrame(rows, columns=['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords', 'Text'])\n",
    "\n",
    "    return topics_df\n",
    "\n",
    "df_topic_keywords = topics_sentences(ldamodel=hdp_model, corpus=corpus, texts=preprocessed_texts)\n",
    "\n",
    "df_topic_keywords.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0640fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Generate df of represtative text for dominant topics for HDP model\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "topics_sorteddf_mallet = pd.DataFrame()\n",
    "topics_outdf_grpd = df_topic_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in topics_outdf_grpd:\n",
    "    topics_sorteddf_mallet = pd.concat([topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
    "\n",
    "# Show\n",
    "topics_sorteddf_mallet.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6c101e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Ploting document word count against nubmer of documents for LDA model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=7, figsize=(20, 4))\n",
    "\n",
    "for i in range(7):\n",
    "    word_counts = []\n",
    "\n",
    "    \n",
    "    for doc in corpus:\n",
    "        # Get the topic distribution for the document\n",
    "        doc_topics = lda_model.get_document_topics(doc)\n",
    "\n",
    "        # Check if the current topic is the dominant topic for the document\n",
    "        for topic, prob in doc_topics:\n",
    "            if topic == i:\n",
    "                # Calculate the word count of the document and add it to the list\n",
    "                word_count = sum(count for _, count in doc)\n",
    "                word_counts.append(word_count)\n",
    "                break\n",
    "\n",
    "    axes[i].hist(word_counts, bins=30, alpha=0.5)\n",
    "    axes[i].set_title(f'Topic {i}')\n",
    "    axes[i].set_xlabel('Document Word Count')\n",
    "    axes[i].set_ylabel('Number of Documents')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06cb098",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Ploting document word count against nubmer of documents for HDP model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=5, figsize=(20, 12))\n",
    "\n",
    "\n",
    "for i in range(42):\n",
    "    word_counts = []\n",
    "\n",
    "    for doc in corpus:\n",
    "        doc_topics = hdp_model[doc]\n",
    "\n",
    "        for topic, prob in doc_topics:\n",
    "            if topic == i:\n",
    "                word_count = sum(count for _, count in doc)\n",
    "                word_counts.append(word_count)\n",
    "                break\n",
    "\n",
    "    # Determine the position of the subplot in the grid\n",
    "    row_index = i // 5\n",
    "    col_index = i % 5\n",
    "\n",
    "    axes[row_index, col_index].hist(word_counts, bins=30, alpha=0.5)\n",
    "    axes[row_index, col_index].set_title(f'Topic {i}')\n",
    "    axes[row_index, col_index].set_xlabel('Document Word Count')\n",
    "    axes[row_index, col_index].set_ylabel('Number of Documents')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5acec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Exploring the relevant terms for each topic of the LDA Model\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=lda_model.id2word)\n",
    "\n",
    "# Convert complex numbers to real numbers in topic coordinates\n",
    "vis.topic_coordinates['x'] = vis.topic_coordinates['x'].apply(lambda x: x.real)\n",
    "vis.topic_coordinates['y'] = vis.topic_coordinates['y'].apply(lambda y: y.real)\n",
    "\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1961c73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Exploring the relevant terms for each topic of the HDP Model\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(hdp_model, corpus, dictionary=dictionary)\n",
    "\n",
    "# Convert complex numbers to real numbers in topic coordinates\n",
    "vis.topic_coordinates['x'] = vis.topic_coordinates['x'].apply(lambda x: x.real)\n",
    "vis.topic_coordinates['y'] = vis.topic_coordinates['y'].apply(lambda y: y.real)\n",
    "\n",
    "vis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
