{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fadcc6c-c439-42ed-9780-bf1aead06cb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T19:44:19.625467Z",
     "iopub.status.busy": "2024-03-31T19:44:19.624646Z",
     "iopub.status.idle": "2024-03-31T19:44:43.407859Z",
     "shell.execute_reply": "2024-03-31T19:44:43.407095Z",
     "shell.execute_reply.started": "2024-03-31T19:44:19.625436Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfminer\n",
      "  Downloading pdfminer-20191125.tar.gz (4.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pdfminer.six\n",
      "  Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gensim\n",
      "  Downloading gensim-4.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting wordcloud\n",
      "  Downloading wordcloud-1.9.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (513 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m513.6/513.6 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pycryptodome\n",
      "  Downloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cryptography>=36.0.0\n",
      "  Downloading cryptography-42.0.5-cp39-abi3-manylinux_2_28_x86_64.whl (4.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m126.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from pdfminer.six) (2.1.1)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in /usr/local/lib/python3.9/dist-packages (from PyPDF2) (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.23.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.9.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from wordcloud) (3.6.1)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from wordcloud) (9.2.0)\n",
      "Collecting pypdfium2>=4.18.0\n",
      "  Downloading pypdfium2-4.28.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m135.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.9/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.15.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (23.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (4.38.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.14.0)\n",
      "Building wheels for collected packages: pdfminer\n",
      "  Building wheel for pdfminer (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pdfminer: filename=pdfminer-20191125-py3-none-any.whl size=6140101 sha256=c87567a4f0a67803a1704ab462e692751713da73769836721f1beeae81410700\n",
      "  Stored in directory: /root/.cache/pip/wheels/41/44/38/6f82831ef593d74608e2f3cca841d5b43b8eb31b6cd60560a9\n",
      "Successfully built pdfminer\n",
      "Installing collected packages: pypdfium2, PyPDF2, pycryptodome, pdfminer, gensim, cryptography, wordcloud, pdfminer.six, pdfplumber\n",
      "Successfully installed PyPDF2-3.0.1 cryptography-42.0.5 gensim-4.3.2 pdfminer-20191125 pdfminer.six-20231228 pdfplumber-0.11.0 pycryptodome-3.20.0 pypdfium2-4.28.0 wordcloud-1.9.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfminer pdfminer.six PyPDF2 gensim wordcloud pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0d4fbf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T19:44:43.410037Z",
     "iopub.status.busy": "2024-03-31T19:44:43.409238Z",
     "iopub.status.idle": "2024-03-31T19:44:47.789824Z",
     "shell.execute_reply": "2024-03-31T19:44:47.789279Z",
     "shell.execute_reply.started": "2024-03-31T19:44:43.410022Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing relevant libraries\n",
    "from pdfminer.high_level import extract_text\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import download\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel\n",
    "import os\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pdfplumber\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import HdpModel\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "# Download other resources\n",
    "download('stopwords')\n",
    "download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a2d4c38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T19:44:47.791687Z",
     "iopub.status.busy": "2024-03-31T19:44:47.790722Z",
     "iopub.status.idle": "2024-03-31T19:44:47.834627Z",
     "shell.execute_reply": "2024-03-31T19:44:47.834176Z",
     "shell.execute_reply.started": "2024-03-31T19:44:47.791660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files in datasource:  1971\n"
     ]
    }
   ],
   "source": [
    "#Initial stats - file count\n",
    "\n",
    "def count_files_in_folder(folder_path):\n",
    "    # Initialize a counter for files\n",
    "    file_count = 0\n",
    "\n",
    "    # Walk through the directory and count files\n",
    "    for _, _, files in os.walk(folder_path):\n",
    "        file_count += len(files)\n",
    "\n",
    "    return file_count\n",
    "\n",
    "folder_path = '/notebooks/CIND820/Datasource'\n",
    "total_files = count_files_in_folder(folder_path)\n",
    "print(\"Total files in datasource: \", total_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94d251db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T19:44:47.836266Z",
     "iopub.status.busy": "2024-03-31T19:44:47.835761Z",
     "iopub.status.idle": "2024-03-31T19:45:41.413415Z",
     "shell.execute_reply": "2024-03-31T19:45:41.412832Z",
     "shell.execute_reply.started": "2024-03-31T19:44:47.836245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages in all PDF files: 155385\n",
      "Mean page count per file: 78.8756345177665\n",
      "Median page count per file: 80.0\n"
     ]
    }
   ],
   "source": [
    "#Initial stats - page count\n",
    "\n",
    "def count_pages_and_stats(folder_path):\n",
    "    total_pages = 0\n",
    "    page_counts = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdf'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'rb') as file:\n",
    "                pdf_reader = PdfReader(file)\n",
    "                num_pages = len(pdf_reader.pages)\n",
    "                total_pages += num_pages\n",
    "                page_counts.append(num_pages)\n",
    "\n",
    "    mean_page_count = statistics.mean(page_counts)\n",
    "    median_page_count = statistics.median(page_counts)\n",
    "\n",
    "    return total_pages, mean_page_count, median_page_count\n",
    "\n",
    "folder_path = '/notebooks/CIND820/Datasource'\n",
    "total_pages, mean_page_count, median_page_count = count_pages_and_stats(folder_path)\n",
    "\n",
    "print(\"Total pages in all PDF files:\", total_pages)\n",
    "print(\"Mean page count per file:\", mean_page_count)\n",
    "print(\"Median page count per file:\", median_page_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "234aa441",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-31T19:45:41.414769Z",
     "iopub.status.busy": "2024-03-31T19:45:41.414582Z",
     "iopub.status.idle": "2024-04-01T02:14:17.130155Z",
     "shell.execute_reply": "2024-04-01T02:14:17.129368Z",
     "shell.execute_reply.started": "2024-03-31T19:45:41.414752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6h 25min 53s, sys: 1min 38s, total: 6h 27min 31s\n",
      "Wall time: 6h 28min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Function for preprocessing text\n",
    "def preprocess_text(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    french_stopwords = set(stopwords.words('french'))\n",
    "    stop_words.update(french_stopwords)\n",
    "    # Remove specific words or letters which are not useful\n",
    "    additional_stopwords = [\n",
    "        'mr.', 'mr', 'mrs.', 'ms.', 'speaker', 'bill', 'debate', 'hon', 'cpc', 'lib', 'bq', 'canadian', \n",
    "        'act', 'amend', 'amendment', 'canada', 'house', 'public', 'honour', 'minister', 'ministry', 'govern', \n",
    "        'member', 'program', 'primeminister', 'would', 'people', 'chair', 'committe', 'liber', 'polici', 'parliamentari', \n",
    "        'ndp', 'government', 'conserv', 'parties', 'partisan', 's', 'b', 'c', 'e', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'o', 'p',\n",
    "        'q', 'r', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'am', 'pm','year','time','motion','go', 'canadians', \n",
    "        'also', 'members', 'madam', 'committee', 'prime', 'senate', 'senator', 'hous',\n",
    "        'one', 'govern', 'liberal', 'conservative', 'liberals', 'conservatives', 'speech', 'parliamentarian',\n",
    "        'secretariat', 'ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'x', 'xi', '000', '1', '3', '5', '11', \n",
    "        '15', '22', '25', '2007', '2008', '2009', '2010', '2011',\n",
    "        '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023', '‚Äô',\n",
    "        '‚Äú', '‚Äù', \"’\",'...................',' ................................................',\n",
    "        '........',\"'s\"]\n",
    "    stop_words.update(additional_stopwords)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Remove numbers, symbols, and certain words\n",
    "    tokens = [re.sub(r'[^a-zA-Z]', '', token) for token in tokens]\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Directory path containing PDF files\n",
    "pdf_directory = '/notebooks/CIND820/Datasource'\n",
    "\n",
    "# List all PDF files in the directory\n",
    "pdf_files = [os.path.join(pdf_directory, file) for file in os.listdir(pdf_directory) if file.endswith('.pdf')]\n",
    "\n",
    "texts = []\n",
    "\n",
    "# Loop through each PDF file and extract text\n",
    "for pdf_file in pdf_files:\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "        texts.append(text)\n",
    "\n",
    "# Preprocess text\n",
    "preprocessed_texts = [preprocess_text(text) for text in texts]\n",
    "\n",
    "# Create a dictionary from the preprocessed text\n",
    "dictionary = Dictionary(preprocessed_texts)\n",
    "\n",
    "# Create a corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in preprocessed_texts]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd108981",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T02:14:17.131597Z",
     "iopub.status.busy": "2024-04-01T02:14:17.131164Z",
     "iopub.status.idle": "2024-04-01T02:14:17.136406Z",
     "shell.execute_reply": "2024-04-01T02:14:17.135915Z",
     "shell.execute_reply.started": "2024-04-01T02:14:17.131579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.31 ms, sys: 6 µs, total: 1.32 ms\n",
      "Wall time: 1.28 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#spliting data for cross validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_corpus, test_corpus = train_test_split(corpus, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df7c7aa2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T02:14:17.137318Z",
     "iopub.status.busy": "2024-04-01T02:14:17.137149Z",
     "iopub.status.idle": "2024-04-01T02:29:27.598725Z",
     "shell.execute_reply": "2024-04-01T02:29:27.597816Z",
     "shell.execute_reply.started": "2024-04-01T02:14:17.137301Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50min 16s, sys: 54min 58s, total: 1h 45min 14s\n",
      "Wall time: 15min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train the HDP model\n",
    "hdp_model = HdpModel(train_corpus, id2word=dictionary, max_chunks=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "589839bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T02:29:27.600125Z",
     "iopub.status.busy": "2024-04-01T02:29:27.599888Z",
     "iopub.status.idle": "2024-04-01T02:31:30.270853Z",
     "shell.execute_reply": "2024-04-01T02:31:30.270249Z",
     "shell.execute_reply.started": "2024-04-01T02:29:27.600081Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 15s, sys: 8min 4s, total: 19min 20s\n",
      "Wall time: 2min 2s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_Index</th>\n",
       "      <th>Topic_Numbers</th>\n",
       "      <th>Topic_Probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 2, 10, 16, 33]</td>\n",
       "      <td>[0.7720939410880234, 0.018150377972648936, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[0, 3, 7]</td>\n",
       "      <td>[0.9330554125850942, 0.04120245056285163, 0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[0, 12]</td>\n",
       "      <td>[0.3035421739169829, 0.6951530081757633]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[0, 17, 24]</td>\n",
       "      <td>[0.8531572782774977, 0.017296391144284842, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[0, 2, 7, 8]</td>\n",
       "      <td>[0.684377702742043, 0.052871333423362386, 0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>[0, 6, 14, 33, 37]</td>\n",
       "      <td>[0.9276987726424688, 0.012705864238219055, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>[0, 3, 7, 8]</td>\n",
       "      <td>[0.12429676545798153, 0.8109624495669837, 0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>[0, 1, 12, 24]</td>\n",
       "      <td>[0.49717136111201093, 0.054415621557670706, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>[0, 13, 36]</td>\n",
       "      <td>[0.8875325378743499, 0.03472889273029267, 0.06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>[0, 35]</td>\n",
       "      <td>[0.9132554554368677, 0.06385120452679714]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_Index       Topic_Numbers  \\\n",
       "0               0  [0, 2, 10, 16, 33]   \n",
       "1               1           [0, 3, 7]   \n",
       "2               2             [0, 12]   \n",
       "3               3         [0, 17, 24]   \n",
       "4               4        [0, 2, 7, 8]   \n",
       "5               5  [0, 6, 14, 33, 37]   \n",
       "6               6        [0, 3, 7, 8]   \n",
       "7               7      [0, 1, 12, 24]   \n",
       "8               8         [0, 13, 36]   \n",
       "9               9             [0, 35]   \n",
       "\n",
       "                                 Topic_Probabilities  \n",
       "0  [0.7720939410880234, 0.018150377972648936, 0.1...  \n",
       "1  [0.9330554125850942, 0.04120245056285163, 0.02...  \n",
       "2           [0.3035421739169829, 0.6951530081757633]  \n",
       "3  [0.8531572782774977, 0.017296391144284842, 0.1...  \n",
       "4  [0.684377702742043, 0.052871333423362386, 0.01...  \n",
       "5  [0.9276987726424688, 0.012705864238219055, 0.0...  \n",
       "6  [0.12429676545798153, 0.8109624495669837, 0.01...  \n",
       "7  [0.49717136111201093, 0.054415621557670706, 0....  \n",
       "8  [0.8875325378743499, 0.03472889273029267, 0.06...  \n",
       "9          [0.9132554554368677, 0.06385120452679714]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#First evaluation of HDP model and number of topics identified per document)\n",
    "rows = []\n",
    "\n",
    "# Iterate through each document in the corpus\n",
    "for i, doc in enumerate(corpus):\n",
    "    doc_topics = hdp_model[doc]\n",
    "    # Extract topic numbers and their probabilities\n",
    "    topic_numbers = [topic[0] for topic in doc_topics]\n",
    "    topic_probs = [topic[1] for topic in doc_topics]\n",
    "    # Append the document's topics to the rows list\n",
    "    rows.append([i, topic_numbers, topic_probs])\n",
    "\n",
    "# Create a DataFrame from the list of rows\n",
    "doc_topics_df = pd.DataFrame(rows, columns=['Document_Index', 'Topic_Numbers', 'Topic_Probabilities'])\n",
    "\n",
    "# Display the DataFrame\n",
    "doc_topics_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc8c96c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T02:31:30.273483Z",
     "iopub.status.busy": "2024-04-01T02:31:30.272965Z",
     "iopub.status.idle": "2024-04-01T02:33:02.321458Z",
     "shell.execute_reply": "2024-04-01T02:33:02.320843Z",
     "shell.execute_reply.started": "2024-04-01T02:31:30.273463Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics identified by HDP model: 42\n",
      "CPU times: user 6min 22s, sys: 4min 35s, total: 10min 57s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# To find an approximate number of total topics identified within the HDP model, I found it easiest to train an\n",
    "#LDA model on the HDP model. \n",
    "# Here we'll train an LDA model using the HDP model as a training mechanism\n",
    "lda_model_t = hdp_model.suggested_lda_model()\n",
    "\n",
    "# Get the topic distributions for each document\n",
    "doc_topics = [lda_model_t.get_document_topics(doc) for doc in corpus]\n",
    "\n",
    "# Count the number of unique topics\n",
    "unique_topics = set()\n",
    "for doc_topics in doc_topics:\n",
    "    unique_topics.update([topic[0] for topic in doc_topics])\n",
    "\n",
    "num_topics_identified = len(unique_topics)\n",
    "print(f\"Number of topics identified by HDP model: {num_topics_identified}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e2f2209",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T02:56:41.225531Z",
     "iopub.status.busy": "2024-04-01T02:56:41.225248Z",
     "iopub.status.idle": "2024-04-01T04:00:05.445436Z",
     "shell.execute_reply": "2024-04-01T04:00:05.444818Z",
     "shell.execute_reply.started": "2024-04-01T02:56:41.225510Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 21min 40s, sys: 46min 24s, total: 2h 8min 5s\n",
      "Wall time: 1h 3min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Crossvalidation using an n-fold analysis showed that the ideal number of topics for LDA was 7. \n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model = LdaModel(train_corpus, id2word=dictionary, num_topics=7, update_every=1, chunksize=10, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cee1400",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T04:00:05.447036Z",
     "iopub.status.busy": "2024-04-01T04:00:05.446606Z",
     "iopub.status.idle": "2024-04-01T04:09:18.562476Z",
     "shell.execute_reply": "2024-04-01T04:09:18.561788Z",
     "shell.execute_reply.started": "2024-04-01T04:00:05.447003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic\tCoherence Value\n",
      "0\t0.3117015089948966\n",
      "1\t0.9384586652166169\n",
      "2\t0.3117015089948966\n",
      "3\t0.3117015089948966\n",
      "4\t0.38490505124552965\n",
      "5\t0.4336867082107532\n",
      "6\t0.23815346092952608\n",
      "CPU times: user 5min 8s, sys: 2min 26s, total: 7min 34s\n",
      "Wall time: 9min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Evaluating LDA topic coherance values. \n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Calculate coherence values for each topic\n",
    "coherence_values = {}\n",
    "for topic_num in range(lda_model.num_topics):\n",
    "    topic_terms = lda_model.show_topic(topic_num)\n",
    "    topic_words = [term for term, _ in topic_terms]\n",
    "    coherence_model = CoherenceModel(topics=[topic_words], texts=preprocessed_texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_values[topic_num] = coherence_model.get_coherence()\n",
    "\n",
    "# Create a table of coherence values\n",
    "print(\"Topic\\tCoherence Value\")\n",
    "for topic_num, coherence_value in coherence_values.items():\n",
    "    print(f\"{topic_num}\\t{coherence_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1bb1aa0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T04:09:18.576997Z",
     "iopub.status.busy": "2024-04-01T04:09:18.576829Z",
     "iopub.status.idle": "2024-04-01T04:11:02.437932Z",
     "shell.execute_reply": "2024-04-01T04:11:02.437360Z",
     "shell.execute_reply.started": "2024-04-01T04:09:18.576969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score for LDA model: 0.510102040764522\n",
      "CPU times: user 46.1 s, sys: 20.3 s, total: 1min 6s\n",
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Finding the overall LDA model coherance value\n",
    "topics = lda_model.show_topics(num_topics=-1, formatted=False)\n",
    "\n",
    "# Calculate coherence values for each topic\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=preprocessed_texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "print(\"Coherence Score for LDA model:\", coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e5688f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-01T04:11:02.438945Z",
     "iopub.status.busy": "2024-04-01T04:11:02.438674Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Evaluating HDP topic coherance values, which were found to have a range below as well as above the LDA model\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "hdp_topics = hdp_model.show_topics(num_topics=42, formatted=False)  # Get the top topics\n",
    "\n",
    "# Extract topic words for each topic\n",
    "topic_words = [[word for word, _ in topic] for topic_id, topic in hdp_topics]\n",
    "\n",
    "# Calculate coherence values for each topic\n",
    "coherence_values = {}\n",
    "for topic_num, words in enumerate(topic_words):\n",
    "    coherence_model = CoherenceModel(topics=[words], texts=preprocessed_texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_values[topic_num] = coherence_model.get_coherence()\n",
    "\n",
    "# Create a table of coherence values\n",
    "print(\"Topic\\tCoherence Value\")\n",
    "for topic_num, coherence_value in coherence_values.items():\n",
    "    print(f\"{topic_num}\\t{coherence_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148e2885",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Finding the overall coherhance value for HDP model\n",
    "hdp_topics = hdp_model.show_topics(num_topics=42, formatted=False)  # Get the top topics\n",
    "\n",
    "# Extract topic words for each topic\n",
    "topic_words = [[word for word, _ in topic] for topic_id, topic in hdp_topics]\n",
    "\n",
    "# Calculate coherence values for each topic\n",
    "coherence_values = {}\n",
    "for topic_num, words in enumerate(topic_words):\n",
    "    coherence_model = CoherenceModel(topics=[words], texts=preprocessed_texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_values[topic_num] = coherence_model.get_coherence()\n",
    "\n",
    "# Compute the average coherence value\n",
    "avg_coherence_value = sum(coherence_values.values()) / len(coherence_values)\n",
    "\n",
    "print(\"Overall Coherence Value for HDP model:\", avg_coherence_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f851d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Generate df with dominant topics, the topic contribution and topic keywords for LDA model\n",
    "import pandas as pd\n",
    "\n",
    "def format_topics(ldamodel=None, corpus=None, texts=None):\n",
    "    # Initialize an empty list to store rows\n",
    "    rows = []\n",
    "\n",
    "    # Iterate through each document in the corpus\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "\n",
    "        # Extract dominant topic, its contribution, and keywords\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # Dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                row_data = [int(topic_num), round(prop_topic, 4), topic_keywords, texts[i]]\n",
    "                rows.append(row_data)\n",
    "                break\n",
    "\n",
    "    # Create df\n",
    "    topics_df = pd.DataFrame(rows, columns=['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords', 'Text'])\n",
    "\n",
    "    return topics_df\n",
    "\n",
    "\n",
    "df_topic_keywords = format_topics(ldamodel=lda_model, corpus=corpus, texts=preprocessed_texts)\n",
    "\n",
    "df_topic_keywords.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75a88f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Generate df of represtative text for dominant topics for LDA model\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "topics_sorteddf_mallet = pd.DataFrame()\n",
    "topics_outdf_grpd = df_topic_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in topics_outdf_grpd:\n",
    "    topics_sorteddf_mallet = pd.concat([topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format DF\n",
    "topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
    "\n",
    "\n",
    "topics_sorteddf_mallet.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e230acf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Generate df with dominant topics, the topic contribution and topic keywords for HDP model\n",
    "import pandas as pd\n",
    "\n",
    "def topics_sentences(ldamodel=None, corpus=None, texts=None):\n",
    "    # Initialize an empty list to store rows\n",
    "    rows = []\n",
    "\n",
    "    # Iterate through each document in the corpus\n",
    "    for i, topics in enumerate(ldamodel[corpus]):\n",
    "        # Sort topics by contribution\n",
    "        topics = sorted(topics, key=lambda x: (x[1]), reverse=True)\n",
    "\n",
    "        # Extract dominant topic, its contribution, and keywords\n",
    "        for j, (topic_num, prop_topic) in enumerate(topics):\n",
    "            if j == 0:  # Dominant topic\n",
    "                topic_keywords = \", \".join([word for word, prop in ldamodel.show_topic(topic_num)])\n",
    "                row_data = [int(topic_num), round(prop_topic, 4), topic_keywords, texts[i]]\n",
    "                rows.append(row_data)\n",
    "                break\n",
    "\n",
    "    # Create df\n",
    "    topics_df = pd.DataFrame(rows, columns=['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords', 'Text'])\n",
    "\n",
    "    return topics_df\n",
    "\n",
    "df_topic_keywords = topics_sentences(ldamodel=hdp_model, corpus=corpus, texts=preprocessed_texts)\n",
    "\n",
    "df_topic_keywords.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0640fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Generate df of represtative text for dominant topics for HDP model\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "topics_sorteddf_mallet = pd.DataFrame()\n",
    "topics_outdf_grpd = df_topic_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in topics_outdf_grpd:\n",
    "    topics_sorteddf_mallet = pd.concat([topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
    "\n",
    "# Show\n",
    "topics_sorteddf_mallet.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6c101e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Ploting document word count against nubmer of documents for LDA model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=7, figsize=(20, 4))\n",
    "\n",
    "for i in range(7):\n",
    "    word_counts = []\n",
    "\n",
    "    \n",
    "    for doc in corpus:\n",
    "        # Get the topic distribution for the document\n",
    "        doc_topics = lda_model.get_document_topics(doc)\n",
    "\n",
    "        # Check if the current topic is the dominant topic for the document\n",
    "        for topic, prob in doc_topics:\n",
    "            if topic == i:\n",
    "                # Calculate the word count of the document and add it to the list\n",
    "                word_count = sum(count for _, count in doc)\n",
    "                word_counts.append(word_count)\n",
    "                break\n",
    "\n",
    "    axes[i].hist(word_counts, bins=30, alpha=0.5)\n",
    "    axes[i].set_title(f'Topic {i}')\n",
    "    axes[i].set_xlabel('Document Word Count')\n",
    "    axes[i].set_ylabel('Number of Documents')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06cb098",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Ploting document word count against nubmer of documents for HDP model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=5, figsize=(20, 12))\n",
    "\n",
    "\n",
    "for i in range(42):\n",
    "    word_counts = []\n",
    "\n",
    "    for doc in corpus:\n",
    "        doc_topics = hdp_model[doc]\n",
    "\n",
    "        for topic, prob in doc_topics:\n",
    "            if topic == i:\n",
    "                word_count = sum(count for _, count in doc)\n",
    "                word_counts.append(word_count)\n",
    "                break\n",
    "\n",
    "    # Determine the position of the subplot in the grid\n",
    "    row_index = i // 5\n",
    "    col_index = i % 5\n",
    "\n",
    "    axes[row_index, col_index].hist(word_counts, bins=30, alpha=0.5)\n",
    "    axes[row_index, col_index].set_title(f'Topic {i}')\n",
    "    axes[row_index, col_index].set_xlabel('Document Word Count')\n",
    "    axes[row_index, col_index].set_ylabel('Number of Documents')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5acec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Exploring the relevant terms for each topic of the LDA Model\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=lda_model.id2word)\n",
    "\n",
    "# Convert complex numbers to real numbers in topic coordinates\n",
    "vis.topic_coordinates['x'] = vis.topic_coordinates['x'].apply(lambda x: x.real)\n",
    "vis.topic_coordinates['y'] = vis.topic_coordinates['y'].apply(lambda y: y.real)\n",
    "\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1961c73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Exploring the relevant terms for each topic of the HDP Model\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(hdp_model, corpus, dictionary=dictionary)\n",
    "\n",
    "# Convert complex numbers to real numbers in topic coordinates\n",
    "vis.topic_coordinates['x'] = vis.topic_coordinates['x'].apply(lambda x: x.real)\n",
    "vis.topic_coordinates['y'] = vis.topic_coordinates['y'].apply(lambda y: y.real)\n",
    "\n",
    "vis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
