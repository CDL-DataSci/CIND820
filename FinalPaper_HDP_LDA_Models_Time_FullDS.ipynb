{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0d4fbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cdlacey/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/cdlacey/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing relevant libraries\n",
    "from pdfminer.high_level import extract_text\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import download\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel\n",
    "import os\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pdfplumber\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import HdpModel\n",
    "\n",
    "\n",
    "\n",
    "# Download other resources\n",
    "download('stopwords')\n",
    "download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a2d4c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files in datasource:  1972\n"
     ]
    }
   ],
   "source": [
    "#Initial stats - file count\n",
    "\n",
    "def count_files_in_folder(folder_path):\n",
    "    # Initialize a counter for files\n",
    "    file_count = 0\n",
    "\n",
    "    # Walk through the directory and count files\n",
    "    for _, _, files in os.walk(folder_path):\n",
    "        file_count += len(files)\n",
    "\n",
    "    return file_count\n",
    "\n",
    "folder_path = '/Users/cdlacey/TMU_DataScience/CIND820/Dataset_Full'\n",
    "total_files = count_files_in_folder(folder_path)\n",
    "print(\"Total files in datasource: \", total_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94d251db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages in all PDF files: 155385\n",
      "Mean page count per file: 78.8756345177665\n",
      "Median page count per file: 80.0\n"
     ]
    }
   ],
   "source": [
    "#Initial stats - page count\n",
    "\n",
    "def count_pages_and_stats(folder_path):\n",
    "    total_pages = 0\n",
    "    page_counts = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdf'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'rb') as file:\n",
    "                pdf_reader = PdfReader(file)\n",
    "                num_pages = len(pdf_reader.pages)\n",
    "                total_pages += num_pages\n",
    "                page_counts.append(num_pages)\n",
    "\n",
    "    mean_page_count = statistics.mean(page_counts)\n",
    "    median_page_count = statistics.median(page_counts)\n",
    "\n",
    "    return total_pages, mean_page_count, median_page_count\n",
    "\n",
    "folder_path = '/Users/cdlacey/TMU_DataScience/CIND820/Dataset_Full'\n",
    "total_pages, mean_page_count, median_page_count = count_pages_and_stats(folder_path)\n",
    "\n",
    "print(\"Total pages in all PDF files:\", total_pages)\n",
    "print(\"Mean page count per file:\", mean_page_count)\n",
    "print(\"Median page count per file:\", median_page_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "234aa441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3h 14min 49s, sys: 1min 41s, total: 3h 16min 30s\n",
      "Wall time: 4h 17min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Function for preprocessing text\n",
    "def preprocess_text(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    french_stopwords = set(stopwords.words('french'))\n",
    "    stop_words.update(french_stopwords)\n",
    "    # Remove specific words or letters which are not useful\n",
    "    additional_stopwords = [\n",
    "        'mr.', 'mr', 'mrs.', 'ms.', 'speaker', 'bill', 'debate', 'hon', 'cpc', 'lib', 'bq', 'canadian', \n",
    "        'act', 'amend', 'amendment', 'canada', 'house', 'public', 'honour', 'minister', 'ministry', 'govern', \n",
    "        'member', 'program', 'primeminister', 'would', 'people', 'chair', 'committe', 'liber', 'polici', 'parliamentari', \n",
    "        'ndp', 'government', 'conserv', 'parties', 'partisan', 's', 'b', 'c', 'e', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'o', 'p',\n",
    "        'q', 'r', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'am', 'pm','year','time','motion','go', 'canadians', \n",
    "        'also', 'members', 'madam', 'committee', 'prime', 'senate', 'senator', 'hous',\n",
    "        'one', 'govern', 'liberal', 'conservative', 'liberals', 'conservatives', 'speech', 'parliamentarian',\n",
    "        'secretariat', 'ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'x', 'xi', '000', '1', '3', '5', '11', \n",
    "        '15', '22', '25', '2007', '2008', '2009', '2010', '2011',\n",
    "        '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023', '‚Äô',\n",
    "        '‚Äú', '‚Äù', \"’\",'...................',' ................................................',\n",
    "        '........',\"'s\"]\n",
    "    stop_words.update(additional_stopwords)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Remove numbers, symbols, and certain words\n",
    "    tokens = [re.sub(r'[^a-zA-Z]', '', token) for token in tokens]\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Directory path containing PDF files\n",
    "pdf_directory = '/Users/cdlacey/TMU_DataScience/CIND820/Dataset_Full'\n",
    "\n",
    "# List all PDF files in the directory\n",
    "pdf_files = [os.path.join(pdf_directory, file) for file in os.listdir(pdf_directory) if file.endswith('.pdf')]\n",
    "\n",
    "texts = []\n",
    "\n",
    "# Loop through each PDF file and extract text\n",
    "for pdf_file in pdf_files:\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "        texts.append(text)\n",
    "\n",
    "# Preprocess text\n",
    "preprocessed_texts = [preprocess_text(text) for text in texts]\n",
    "\n",
    "# Create a dictionary from the preprocessed text\n",
    "dictionary = Dictionary(preprocessed_texts)\n",
    "\n",
    "# Create a corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in preprocessed_texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd108981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.14 ms, sys: 3.99 ms, total: 5.14 ms\n",
      "Wall time: 8.31 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#spliting data for cross validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_corpus, test_corpus = train_test_split(corpus, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df7c7aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44min 44s, sys: 16min 29s, total: 1h 1min 13s\n",
      "Wall time: 35min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train the HDP model\n",
    "hdp_model = HdpModel(train_corpus, id2word=dictionary, max_chunks=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "589839bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24min 5s, sys: 1min 48s, total: 25min 54s\n",
      "Wall time: 3min 55s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_Index</th>\n",
       "      <th>Topic_Numbers</th>\n",
       "      <th>Topic_Probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 4, 21, 33]</td>\n",
       "      <td>[0.8695607679003938, 0.03150255851760758, 0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0.9999366267621004]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>[0.9834829294458596, 0.01469811157675135]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[0, 2, 6, 8, 12, 18]</td>\n",
       "      <td>[0.34035409751587276, 0.16740626525441996, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[0, 8, 13, 23, 37, 38]</td>\n",
       "      <td>[0.8665857310162866, 0.02421795733086295, 0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[0.9998972164761384]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[0.9998613556424483]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>[0, 3, 12, 19]</td>\n",
       "      <td>[0.5562241015015653, 0.23732829801384997, 0.17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0.9998740157868754]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>[0, 26]</td>\n",
       "      <td>[0.9763880688346864, 0.015923120537856616]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_Index           Topic_Numbers  \\\n",
       "0               0          [0, 4, 21, 33]   \n",
       "1               1                     [0]   \n",
       "2               2                  [0, 2]   \n",
       "3               3    [0, 2, 6, 8, 12, 18]   \n",
       "4               4  [0, 8, 13, 23, 37, 38]   \n",
       "5               5                     [2]   \n",
       "6               6                     [1]   \n",
       "7               7          [0, 3, 12, 19]   \n",
       "8               8                     [0]   \n",
       "9               9                 [0, 26]   \n",
       "\n",
       "                                 Topic_Probabilities  \n",
       "0  [0.8695607679003938, 0.03150255851760758, 0.01...  \n",
       "1                               [0.9999366267621004]  \n",
       "2          [0.9834829294458596, 0.01469811157675135]  \n",
       "3  [0.34035409751587276, 0.16740626525441996, 0.0...  \n",
       "4  [0.8665857310162866, 0.02421795733086295, 0.02...  \n",
       "5                               [0.9998972164761384]  \n",
       "6                               [0.9998613556424483]  \n",
       "7  [0.5562241015015653, 0.23732829801384997, 0.17...  \n",
       "8                               [0.9998740157868754]  \n",
       "9         [0.9763880688346864, 0.015923120537856616]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#First evaluation of HDP model and number of topics identified per document)\n",
    "rows = []\n",
    "\n",
    "# Iterate through each document in the corpus\n",
    "for i, doc in enumerate(corpus):\n",
    "    doc_topics = hdp_model[doc]\n",
    "    # Extract topic numbers and their probabilities\n",
    "    topic_numbers = [topic[0] for topic in doc_topics]\n",
    "    topic_probs = [topic[1] for topic in doc_topics]\n",
    "    # Append the document's topics to the rows list\n",
    "    rows.append([i, topic_numbers, topic_probs])\n",
    "\n",
    "# Create a DataFrame from the list of rows\n",
    "doc_topics_df = pd.DataFrame(rows, columns=['Document_Index', 'Topic_Numbers', 'Topic_Probabilities'])\n",
    "\n",
    "# Display the DataFrame\n",
    "doc_topics_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc8c96c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics identified by HDP model: 40\n",
      "CPU times: user 12min 4s, sys: 43.2 s, total: 12min 47s\n",
      "Wall time: 3min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# To find an approximate number of total topics identified within the HDP model, I found it easiest to train an\n",
    "#LDA model on the HDP model. \n",
    "# Here we'll train an LDA model using the HDP model as a training mechanism\n",
    "lda_model_t = hdp_model.suggested_lda_model()\n",
    "\n",
    "# Get the topic distributions for each document\n",
    "doc_topics = [lda_model_t.get_document_topics(doc) for doc in corpus]\n",
    "\n",
    "# Count the number of unique topics\n",
    "unique_topics = set()\n",
    "for doc_topics in doc_topics:\n",
    "    unique_topics.update([topic[0] for topic in doc_topics])\n",
    "\n",
    "num_topics_identified = len(unique_topics)\n",
    "print(f\"Number of topics identified by HDP model: {num_topics_identified}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e2f2209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 8min 34s, sys: 35min 28s, total: 1h 44min 3s\n",
      "Wall time: 31min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Crossvalidation using an n-fold analysis showed that the ideal number of topics for LDA was 7. \n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model = LdaModel(train_corpus, id2word=dictionary, num_topics=7, update_every=1, chunksize=10, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cee1400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic\tCoherence Value\n",
      "0\t0.666741048974939\n",
      "1\t0.3906467298239501\n",
      "2\t0.666741048974939\n",
      "3\t0.666741048974939\n",
      "4\t0.43368670032036294\n",
      "5\t0.666741048974939\n",
      "6\t0.36582895413459704\n",
      "CPU times: user 2min 38s, sys: 59.8 s, total: 3min 38s\n",
      "Wall time: 7min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Evaluating LDA topic coherance values. \n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Calculate coherence values for each topic\n",
    "coherence_values = {}\n",
    "for topic_num in range(lda_model.num_topics):\n",
    "    topic_terms = lda_model.show_topic(topic_num)\n",
    "    topic_words = [term for term, _ in topic_terms]\n",
    "    coherence_model = CoherenceModel(topics=[topic_words], texts=preprocessed_texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_values[topic_num] = coherence_model.get_coherence()\n",
    "\n",
    "# Create a table of coherence values\n",
    "print(\"Topic\\tCoherence Value\")\n",
    "for topic_num, coherence_value in coherence_values.items():\n",
    "    print(f\"{topic_num}\\t{coherence_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1bb1aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score for LDA model: 0.628447487531789\n",
      "CPU times: user 23.8 s, sys: 7.84 s, total: 31.6 s\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Finding the overall LDA model coherance value\n",
    "topics = lda_model.show_topics(num_topics=-1, formatted=False)\n",
    "\n",
    "# Calculate coherence values for each topic\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=preprocessed_texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "print(\"Coherence Score for LDA model:\", coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63e5688f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic\tCoherence Value\n",
      "0\t0.3364034848838516\n",
      "1\t0.3364034848838516\n",
      "2\t0.27515213660701904\n",
      "3\t0.2823407747517962\n",
      "4\t0.3173204592285453\n",
      "5\t0.332263459312759\n",
      "6\t0.31918224100633885\n",
      "7\t0.33455076422329055\n",
      "8\t0.3203299259662332\n",
      "9\t0.3369178263808213\n",
      "10\t0.2721410562804787\n",
      "11\t0.3103699555446524\n",
      "12\t0.278618061830709\n",
      "13\t0.34067572471918883\n",
      "14\t0.3822184438060659\n",
      "15\t0.3390258108252455\n",
      "16\t0.33064272464139866\n",
      "17\t0.3224578343299504\n",
      "18\t0.2981939262392516\n",
      "19\t0.22637349669686233\n",
      "20\t0.3505345368804139\n",
      "21\t0.30109477441853305\n",
      "22\t0.3227684244172857\n",
      "23\t0.31615488486119153\n",
      "24\t0.34339287121134515\n",
      "25\t0.3191801016694206\n",
      "26\t0.31279899142772305\n",
      "27\t0.2991498848665687\n",
      "28\t0.2748779326676663\n",
      "29\t0.40894770082049225\n",
      "30\t0.29375181315014204\n",
      "31\t0.29683283358371504\n",
      "32\t0.3468637657403343\n",
      "33\t0.291993120979651\n",
      "34\t0.34823557380634196\n",
      "35\t0.3752695938823106\n",
      "36\t0.3347474857779783\n",
      "37\t0.31153236572642146\n",
      "CPU times: user 16min 9s, sys: 4min 57s, total: 21min 7s\n",
      "Wall time: 54min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Evaluating HDP topic coherance values, which were found to have a range below as well as above the LDA model\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "hdp_topics = hdp_model.show_topics(num_topics=38, formatted=False)  # Get the top topics\n",
    "\n",
    "# Extract topic words for each topic\n",
    "topic_words = [[word for word, _ in topic] for topic_id, topic in hdp_topics]\n",
    "\n",
    "# Calculate coherence values for each topic\n",
    "coherence_values = {}\n",
    "for topic_num, words in enumerate(topic_words):\n",
    "    coherence_model = CoherenceModel(topics=[words], texts=preprocessed_texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_values[topic_num] = coherence_model.get_coherence()\n",
    "\n",
    "# Create a table of coherence values\n",
    "print(\"Topic\\tCoherence Value\")\n",
    "for topic_num, coherence_value in coherence_values.items():\n",
    "    print(f\"{topic_num}\\t{coherence_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "148e2885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Coherence Value for HDP model: 0.3194660065275221\n",
      "CPU times: user 16min 10s, sys: 5min 2s, total: 21min 13s\n",
      "Wall time: 1h 2min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Finding the overall coherhance value for HDP model\n",
    "hdp_topics = hdp_model.show_topics(num_topics=38, formatted=False)  # Get the top topics\n",
    "\n",
    "# Extract topic words for each topic\n",
    "topic_words = [[word for word, _ in topic] for topic_id, topic in hdp_topics]\n",
    "\n",
    "# Calculate coherence values for each topic\n",
    "coherence_values = {}\n",
    "for topic_num, words in enumerate(topic_words):\n",
    "    coherence_model = CoherenceModel(topics=[words], texts=preprocessed_texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_values[topic_num] = coherence_model.get_coherence()\n",
    "\n",
    "# Compute the average coherence value\n",
    "avg_coherence_value = sum(coherence_values.values()) / len(coherence_values)\n",
    "\n",
    "print(\"Overall Coherence Value for HDP model:\", avg_coherence_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f851d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 48s, sys: 10min 13s, total: 19min 2s\n",
      "Wall time: 2min 32s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Perc_Contribution</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.8253</td>\n",
       "      <td>commonsdeb, work, nation, english, order, tran...</td>\n",
       "      <td>[common, debat, volum, number, session, parlia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.9941</td>\n",
       "      <td>need, work, support, want, make, right, get, l...</td>\n",
       "      <td>[parliament, session, common, debat, offici, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.9924</td>\n",
       "      <td>need, work, support, want, make, right, get, l...</td>\n",
       "      <td>[parliament, session, common, debat, offici, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9989</td>\n",
       "      <td>commonsdeb, work, nation, english, order, tran...</td>\n",
       "      <td>[common, debat, volum, number, session, parlia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9450</td>\n",
       "      <td>commonsdeb, work, nation, english, order, tran...</td>\n",
       "      <td>[common, debat, volum, number, session, parlia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0.5582</td>\n",
       "      <td>commonsdeb, work, nation, english, order, tran...</td>\n",
       "      <td>[common, debat, volum, number, session, parlia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9726</td>\n",
       "      <td>commonsdeb, work, nation, english, order, tran...</td>\n",
       "      <td>[common, debat, volum, number, session, parlia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0.8392</td>\n",
       "      <td>commonsdeb, work, nation, english, order, tran...</td>\n",
       "      <td>[common, debat, volum, number, session, parlia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>0.9684</td>\n",
       "      <td>need, work, support, want, make, right, get, l...</td>\n",
       "      <td>[parliament, session, common, debat, offici, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0.7756</td>\n",
       "      <td>commonsdeb, work, nation, english, order, tran...</td>\n",
       "      <td>[common, debat, volum, number, session, parlia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dominant_Topic  Perc_Contribution  \\\n",
       "0               1             0.8253   \n",
       "1               4             0.9941   \n",
       "2               4             0.9924   \n",
       "3               1             0.9989   \n",
       "4               1             0.9450   \n",
       "5               1             0.5582   \n",
       "6               1             0.9726   \n",
       "7               1             0.8392   \n",
       "8               4             0.9684   \n",
       "9               1             0.7756   \n",
       "\n",
       "                                      Topic_Keywords  \\\n",
       "0  commonsdeb, work, nation, english, order, tran...   \n",
       "1  need, work, support, want, make, right, get, l...   \n",
       "2  need, work, support, want, make, right, get, l...   \n",
       "3  commonsdeb, work, nation, english, order, tran...   \n",
       "4  commonsdeb, work, nation, english, order, tran...   \n",
       "5  commonsdeb, work, nation, english, order, tran...   \n",
       "6  commonsdeb, work, nation, english, order, tran...   \n",
       "7  commonsdeb, work, nation, english, order, tran...   \n",
       "8  need, work, support, want, make, right, get, l...   \n",
       "9  commonsdeb, work, nation, english, order, tran...   \n",
       "\n",
       "                                                Text  \n",
       "0  [common, debat, volum, number, session, parlia...  \n",
       "1  [parliament, session, common, debat, offici, r...  \n",
       "2  [parliament, session, common, debat, offici, r...  \n",
       "3  [common, debat, volum, number, session, parlia...  \n",
       "4  [common, debat, volum, number, session, parlia...  \n",
       "5  [common, debat, volum, number, session, parlia...  \n",
       "6  [common, debat, volum, number, session, parlia...  \n",
       "7  [common, debat, volum, number, session, parlia...  \n",
       "8  [parliament, session, common, debat, offici, r...  \n",
       "9  [common, debat, volum, number, session, parlia...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#Generate df with dominant topics, the topic contribution and topic keywords for LDA model\n",
    "import pandas as pd\n",
    "\n",
    "def format_topics(ldamodel=None, corpus=None, texts=None):\n",
    "    # Initialize an empty list to store rows\n",
    "    rows = []\n",
    "\n",
    "    # Iterate through each document in the corpus\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "\n",
    "        # Extract dominant topic, its contribution, and keywords\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # Dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                row_data = [int(topic_num), round(prop_topic, 4), topic_keywords, texts[i]]\n",
    "                rows.append(row_data)\n",
    "                break\n",
    "\n",
    "    # Create df\n",
    "    topics_df = pd.DataFrame(rows, columns=['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords', 'Text'])\n",
    "\n",
    "    return topics_df\n",
    "\n",
    "\n",
    "df_topic_keywords = format_topics(ldamodel=lda_model, corpus=corpus, texts=preprocessed_texts)\n",
    "\n",
    "df_topic_keywords.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f75a88f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 220 ms, sys: 199 ms, total: 419 ms\n",
      "Wall time: 70.3 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Num</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Representative Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>commonsdeb, work, nation, english, order, translat, right, countri, first, like</td>\n",
       "      <td>[common, debat, volum, number, session, parliament, offici, report, hansard, tuesday, decemb, ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>need, work, support, want, make, right, get, like, common, know</td>\n",
       "      <td>[parliament, session, common, debat, offici, report, hansard, volum, thursday, decemb, honour, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0.7902</td>\n",
       "      <td>ontario, qubec, britishcolumbia, theminist, tom, deanallison, davidsweet, joepreston, jacquesgou...</td>\n",
       "      <td>[common, debat, volum, number, session, parliament, offici, report, hansard, friday, april, hono...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic_Num  Topic_Perc_Contrib  \\\n",
       "0          1              0.9996   \n",
       "1          4              1.0000   \n",
       "2          6              0.7902   \n",
       "\n",
       "                                                                                              Keywords  \\\n",
       "0                      commonsdeb, work, nation, english, order, translat, right, countri, first, like   \n",
       "1                                      need, work, support, want, make, right, get, like, common, know   \n",
       "2  ontario, qubec, britishcolumbia, theminist, tom, deanallison, davidsweet, joepreston, jacquesgou...   \n",
       "\n",
       "                                                                                   Representative Text  \n",
       "0  [common, debat, volum, number, session, parliament, offici, report, hansard, tuesday, decemb, ho...  \n",
       "1  [parliament, session, common, debat, offici, report, hansard, volum, thursday, decemb, honour, g...  \n",
       "2  [common, debat, volum, number, session, parliament, offici, report, hansard, friday, april, hono...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#Generate df of represtative text for dominant topics for LDA model\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "topics_sorteddf_mallet = pd.DataFrame()\n",
    "topics_outdf_grpd = df_topic_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in topics_outdf_grpd:\n",
    "    topics_sorteddf_mallet = pd.concat([topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format DF\n",
    "topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
    "\n",
    "\n",
    "topics_sorteddf_mallet.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
